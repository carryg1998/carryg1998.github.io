<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-08-12T08:44:28.482Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>CarryG</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>swin-transformer学习</title>
    <link href="http://yoursite.com/2021/08/07/swin-transformer%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2021/08/07/swin-transformer%E5%AD%A6%E4%B9%A0/</id>
    <published>2021-08-07T09:55:42.000Z</published>
    <updated>2021-08-12T08:44:28.482Z</updated>
    
    <content type="html"><![CDATA[<p>由于知识点实在是太多了，做一下swin-transformer学习的记录</p><a id="more"></a><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>swin-transformer是在vit提出以后的升级版，传统的vit有几个问题：</p><p>1.图像分辨率增加的话，像素点变多attention的计算会大大的增加</p><p>2.由于最终结果分辨率不够，不适合于做object-detection的任务</p><p>原文是这样说的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">These qualities of Swin Transformer make it compatible with a broad range of vision tasks,including image classification and dense prediction tasks such as object detection and semantic segmentation</span><br></pre></td></tr></table></figure><p>改进后的swin-transformer可以进行深度预测以及目标检测的工作。</p><p>swin-transformer提出了将像素分窗口计算的想法，特征图切割后单独计算attention，减少了计算量。</p><p><img src="/2021/08/07/swin-transformer学习/0.png" alt="swin-transformer示意图"></p><p>更高分辨率的像素点也更加适合做目标检测的工作。</p><h2 id="2-SWT结构"><a href="#2-SWT结构" class="headerlink" title="2.SWT结构"></a>2.SWT结构</h2><p><img src="/2021/08/07/swin-transformer学习/1.png" alt="swin-transformer整体结构"></p><p>整体还是层次化的设计，每一层stage的结束都会有一个patch merging作为下采样，缩小特征图的分辨率扩大感受野。</p><p>开始的patch embedding与VIT的结构一样，将图像切割成多块后再展平形成多个token</p><h3 id="patch-embedding"><a href="#patch-embedding" class="headerlink" title="patch embedding"></a>patch embedding</h3><p><img src="/2021/08/07/swin-transformer学习/2.png" alt="patch embedding"></p><p>他的实现也非常简单，利用一层卷积核和步长均为patch size的卷积层就可以完成划分了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">class patch_embd(nn.Module):</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    [b,c,h,w] -&gt; [b,fc,fh,fw]</span><br><span class="line">    利用卷积核和步长为为patch_size的卷积实现patch embedding</span><br><span class="line">    最终得到特征图长*宽个token向量</span><br><span class="line">    Args:</span><br><span class="line">        img_size:[特征图长度, 特征图宽度]</span><br><span class="line">        patch_size:每一个token向量的长宽</span><br><span class="line">        norm:可以选择layernorm或者batchnorm</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    def __init__(self, img_size, patch_size, in_channel, out_channel, norm=None):</span><br><span class="line">        super(patch_embd, self).__init__()</span><br><span class="line">        self.img_length = img_size[0]</span><br><span class="line">        self.img_width = img_size[1]</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line"></span><br><span class="line">        self.conv_embd = nn.Conv2d(in_channel, out_channel, kernel_size=patch_size, stride=patch_size)</span><br><span class="line"></span><br><span class="line">        if norm is not None:</span><br><span class="line">            self.norm = norm(out_channel)</span><br><span class="line">        else:</span><br><span class="line">            self.norm = None</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        out = self.conv_embd(x)</span><br><span class="line">        if self.norm is not None:</span><br><span class="line">            out = self.norm(out)</span><br><span class="line">        return out</span><br></pre></td></tr></table></figure><h3 id="Window-Partition-Reverse"><a href="#Window-Partition-Reverse" class="headerlink" title="Window Partition/Reverse"></a>Window Partition/Reverse</h3><p>此操作就是将原本的特征图进行窗口的划分</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def window_partition(x, window_size):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    将token划分为多个window，[b,c,h,w] -&gt; [b, c, -1, window_size, window_size]</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    B, C, H, W = x.shape</span><br><span class="line">    x = x.view(B, C, H // window_size, window_size, W // window_size, window_size)</span><br><span class="line">    windows = x.permute(0, 1, 2, 4, 3, 5).contiguous().view(B, C, -1, window_size, window_size)</span><br><span class="line">    return windows</span><br><span class="line"></span><br><span class="line">def window_reverse(windows, window_size, H, W):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    还原窗口化</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    B, C = windows.shape[0], windows.shape[1]</span><br><span class="line">    x = windows.view(B, C, H // window_size, W // window_size, window_size, window_size)</span><br><span class="line">    x = x.permute(0, 1, 2, 4, 3, 5).contiguous().view(B, C, H, W)</span><br><span class="line">    return x</span><br></pre></td></tr></table></figure><h3 id="Windows-Attention"><a href="#Windows-Attention" class="headerlink" title="Windows Attention"></a>Windows Attention</h3><p>swin-transformer的核心之一，传统的vit中注意力的计算都是基于整个特征图的，如果特征图的分辨率增加将会产生很大的计算量，SWT将特征图进行分割，使注意力的计算限制在小的窗口内，减小计算量。</p><p>公式如下：</p><script type="math/tex; mode=display">\mbox{Attention(Q,K,V)}=\mbox{Softmax($\frac{QK^T}{\sqrt{d}}$+B)}V</script><p>Attention的计算代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def Attention(q, k):</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    softmax(Q*T(K)/sqrt(dim))</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    out = q.matmul(k.transpose(-2, -1))/math.sqrt(k.shape[-1])</span><br><span class="line">    out = out.softmax(dim=1)</span><br><span class="line">    return out</span><br></pre></td></tr></table></figure><p>Windows Attention每个维度为[Batch,channel,windows的数量,每个windows的token数,token长度]</p><p>（其中的mask后文会提到）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">class Windows_Attention(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    按窗口计算Attention</span><br><span class="line">    input:[B, C, nW, hW, wW]</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, in_channel, windows_size, num_heads, drop_ratio=0.):</span><br><span class="line">        super(Windows_Attention, self).__init__()</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.windows_size = windows_size</span><br><span class="line">        self.weight_q = nn.Linear(in_channel, in_channel, bias=True)</span><br><span class="line">        self.weight_k = nn.Linear(in_channel, in_channel, bias=True)</span><br><span class="line">        self.weight_v = nn.Linear(in_channel, in_channel, bias=True)</span><br><span class="line"></span><br><span class="line">        # 定义位置偏置</span><br><span class="line">        self.relative_position_bias_table = nn.Parameter(</span><br><span class="line">            torch.zeros((2 * self.windows_size[0] - 1) * (2 * self.windows_size[1] - 1), num_heads))</span><br><span class="line"></span><br><span class="line">        # 定义位置编码</span><br><span class="line">        coords_h = torch.arange(self.windows_size[0])</span><br><span class="line">        coords_w = torch.arange(self.windows_size[1])</span><br><span class="line">        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww</span><br><span class="line">        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww</span><br><span class="line">        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww</span><br><span class="line">        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2</span><br><span class="line">        relative_coords[:, :, 0] += self.windows_size[0] - 1  # shift to start from 0</span><br><span class="line">        relative_coords[:, :, 1] += self.windows_size[1] - 1</span><br><span class="line">        relative_coords[:, :, 0] *= 2 * self.windows_size[1] - 1</span><br><span class="line">        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww</span><br><span class="line">        self.register_buffer(&quot;relative_position_index&quot;, relative_position_index)</span><br><span class="line"></span><br><span class="line">        self.softmax = nn.Softmax(dim=-1)</span><br><span class="line">        self.atten_drop = nn.Dropout(drop_ratio)</span><br><span class="line">        self.fc = nn.Linear(in_channel, in_channel)</span><br><span class="line">        self.fc_drop = nn.Dropout(drop_ratio)</span><br><span class="line"></span><br><span class="line">    def forward(self, x, mask=None):</span><br><span class="line">        B, C, nW, HW, WW = x.shape</span><br><span class="line">        N = HW*WW</span><br><span class="line">        x = x.permute(0, 2, 3, 4, 1).view(B, nW, N, C)</span><br><span class="line">        # [b,768,h*w] -&gt; [b， h*w/head, head, 768]</span><br><span class="line">        q = self.weight_q(x)</span><br><span class="line">        k = self.weight_k(x)</span><br><span class="line">        v = self.weight_v(x)</span><br><span class="line">        q = q.reshape(B, nW, self.num_heads, N, C // self.num_heads)</span><br><span class="line">        k = k.reshape(B, nW, self.num_heads, N, C // self.num_heads)</span><br><span class="line">        v = v.reshape(B, nW, self.num_heads, N, C // self.num_heads)</span><br><span class="line"></span><br><span class="line">        atten = Attention(q, k)</span><br><span class="line"></span><br><span class="line">        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(</span><br><span class="line">            self.windows_size[0] * self.windows_size[1], self.windows_size[0] * self.windows_size[1], -1)</span><br><span class="line">        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww</span><br><span class="line">        atten = atten + relative_position_bias.unsqueeze(0)</span><br><span class="line"></span><br><span class="line">        if mask is not None:</span><br><span class="line">            atten = atten + mask.unsqueeze(1).unsqueeze(0)</span><br><span class="line"></span><br><span class="line">        atten = self.softmax(atten)</span><br><span class="line">        atten = self.atten_drop(atten)</span><br><span class="line"></span><br><span class="line">        x = atten.matmul(v)</span><br><span class="line">        x = x.reshape(B, nW, HW, WW, C)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        out = self.fc_drop(x).permute(0, 4, 1, 2, 3).contiguous()</span><br><span class="line">        return out</span><br></pre></td></tr></table></figure><h3 id="Shifted-Window-Attention"><a href="#Shifted-Window-Attention" class="headerlink" title="Shifted Window Attention"></a>Shifted Window Attention</h3><p>前文提到我们可以分窗口计算减小Attention的计算量，但是这会导致被分割掉的区域无法计算Attention，原文就提出了窗口转换的想法，以此来保证特征图的每一片区域都可以计算的注意力。</p><p><img src="/2021/08/07/swin-transformer学习/3.png" alt="shifted windows"></p><p>如图我们可以看到由此将原本的特征图分割成了九个窗口，随后我们在将特征图进行滚动(roll)，变为之前的四个特征图的形状。</p><p><img src="/2021/08/07/swin-transformer学习/4.png" alt="roll"></p><p>但是这样的话计算的就不是按原本的9个窗口计算自己窗口内的注意力了，不同的窗口所计算的注意力就会混杂在一起。原文提出了一种掩码的方式来解决，虽然原文介绍不详细，但是看代码就可以很简单的理解了。</p><p>如下图所示（以下图源公众号：GiantPandaCV）：</p><p><img src="/2021/08/07/swin-transformer学习/5.png" alt="mask"></p><p>我们首先都每个窗口进行编号，右图为滚动以后每个编号窗口的位置。我们要的是将Attention的计算限制在每个窗口内，每个部分的计算如下，我们在计算self-Attention时先把它展平为Q，然后转置为T，最后得到的结果分布如图所示：</p><p><img src="/2021/08/07/swin-transformer学习/6.png" alt="mask"></p><p>我们只需取出对应的位置的值就可以了(其他位置都是不同窗口相乘的结果了)</p><p>那这个掩码具体是如何实现的呢？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">if self.shifited_size &gt; 0:</span><br><span class="line"># calculate attention mask for SW-MSA</span><br><span class="line">     H, W = self.input_size</span><br><span class="line">     img_mask = torch.zeros((1, 1, H, W))  # 1 H W 1</span><br><span class="line">     h_slices = (slice(0, -self.windows_size),</span><br><span class="line">          slice(-self.windows_size, -self.shifited_size),</span><br><span class="line">         slice(-self.shifited_size, None))</span><br><span class="line">     w_slices = (slice(0, -self.windows_size),</span><br><span class="line">                 slice(-self.windows_size, -self.shifited_size),</span><br><span class="line">                 slice(-self.shifited_size, None))</span><br><span class="line">     cnt = 0</span><br><span class="line">     for h in h_slices:</span><br><span class="line">         for w in w_slices:</span><br><span class="line">            img_mask[:, :, h, w] = cnt</span><br><span class="line">            cnt += 1</span><br><span class="line"></span><br><span class="line">     mask_windows = window_partition(img_mask, self.windows_size)  # nW, window_size, window_size, 1</span><br><span class="line">     mask_windows = mask_windows.view(-1, self.windows_size * self.windows_size)</span><br><span class="line">     attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)</span><br><span class="line">     attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))</span><br><span class="line">else:</span><br><span class="line">     attn_mask = None</span><br></pre></td></tr></table></figure><p>由上图的代码我们可以生成这样的代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[[   0.,    0.,    0.,    0.],</span><br><span class="line">           [   0.,    0.,    0.,    0.],</span><br><span class="line">           [   0.,    0.,    0.,    0.],</span><br><span class="line">           [   0.,    0.,    0.,    0.]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">         [[[   0., -100.,    0., -100.],</span><br><span class="line">           [-100.,    0., -100.,    0.],</span><br><span class="line">           [   0., -100.,    0., -100.],</span><br><span class="line">           [-100.,    0., -100.,    0.]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">         [[[   0.,    0., -100., -100.],</span><br><span class="line">           [   0.,    0., -100., -100.],</span><br><span class="line">           [-100., -100.,    0.,    0.],</span><br><span class="line">           [-100., -100.,    0.,    0.]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">         [[[   0., -100., -100., -100.],</span><br><span class="line">           [-100.,    0., -100., -100.],</span><br><span class="line">           [-100., -100.,    0., -100.],</span><br><span class="line">           [-100., -100., -100.,    0.]]]]])</span><br></pre></td></tr></table></figure><p>我们在不需要的位置上置-100与原本的特征图相加并传入softmax，softmax会忽略掉极小的特征值。</p><p><img src="/2021/08/07/swin-transformer学习/7.png" alt="softmax函数"></p><h3 id="Patch-merging"><a href="#Patch-merging" class="headerlink" title="Patch merging"></a>Patch merging</h3><p>在计算完注意力之后，SWT构思了Patch merging的下采样方式，有点类似于yolov5中的focus结构，就是将每4个特征点为一组，取对应位置的值拼接，最后全部堆叠在一起，做到下采样的效果，显而易见，操作后会使特征图长宽减小一半，通道数变为原来的四倍。</p><p><img src="/2021/08/07/swin-transformer学习/8.png" alt="Patch merging"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">class PatchMerging(nn.Module):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    PatchMerging层</span><br><span class="line">    [B,C,H,W] -&gt; [B, C*4, H/2, W/2]</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, input_size, in_channel, norm_layer=nn.LayerNorm):</span><br><span class="line">        super(PatchMerging, self).__init__()</span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.in_channel = in_channel</span><br><span class="line">        self.fc = nn.Linear(4 * in_channel, 2 * in_channel, bias=False)</span><br><span class="line">        self.norm = norm_layer(4 * in_channel)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        H, W = self.input_size</span><br><span class="line">        B, C, h, w = x.shape</span><br><span class="line"></span><br><span class="line">        x = x.permute(0, 2, 3, 1)</span><br><span class="line">        x0 = x[:, 0::2, 0::2, :]</span><br><span class="line">        x1 = x[:, 1::2, 0::2, :]</span><br><span class="line">        x2 = x[:, 0::2, 1::2, :]</span><br><span class="line">        x3 = x[:, 1::2, 1::2, :]</span><br><span class="line">        x = torch.cat([x0, x1, x2, x3], -1)</span><br><span class="line">        x = x.view(B, -1, 4 * C)</span><br><span class="line"></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        out = x.transpose(-1, -2).contiguous().view(B, 2 * C, h//2, w//2)</span><br><span class="line">        return out</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由于知识点实在是太多了，做一下swin-transformer学习的记录&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="swin-transformer" scheme="http://yoursite.com/tags/swin-transformer/"/>
    
      <category term="detection" scheme="http://yoursite.com/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>mmdetection配置</title>
    <link href="http://yoursite.com/2021/08/05/mmdetection%E9%85%8D%E7%BD%AE/"/>
    <id>http://yoursite.com/2021/08/05/mmdetection%E9%85%8D%E7%BD%AE/</id>
    <published>2021-08-05T02:14:19.000Z</published>
    <updated>2021-08-05T02:46:18.574Z</updated>
    
    <content type="html"><![CDATA[<p>水一篇在linux上搭建mmdetection的博客<br><a id="more"></a></p><h2 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h2><h3 id="1-从github上克隆源码"><a href="#1-从github上克隆源码" class="headerlink" title="1.从github上克隆源码"></a>1.从github上克隆源码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/open-mmlab/mmdetection.git</span><br><span class="line">cd mmdetection-master</span><br></pre></td></tr></table></figure><h3 id="2-安装requires环境"><a href="#2-安装requires环境" class="headerlink" title="2.安装requires环境"></a>2.安装requires环境</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><h3 id="3-运行setup配置"><a href="#3-运行setup配置" class="headerlink" title="3.运行setup配置"></a>3.运行setup配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python setup.py develop</span><br></pre></td></tr></table></figure><h2 id="运行测试demo"><a href="#运行测试demo" class="headerlink" title="运行测试demo"></a>运行测试demo</h2><p>在mmdetection的根目录下新建checkpoints文件夹用来存放模型权重，权重文件在configs文件夹中找到相用的模型，比如我要使用yolo模型，直接打开yolo的文件夹，找到其中的metafile.yml文件中有模型权重地址，下载到checkpoints中</p><p><img src="/2021/08/05/mmdetection配置/0.png" alt="0"></p><p>选择好对应的模型后利用demo文件夹里的图片就可以进行测试了，我选择的是yolov3_d53_mstrain-416_273e_coco(darknet53backbone，输入是416，coco数据集预训练的权重)</p><p>运行以下代码python {测试代码} {测试图片} {模型代码} {权重位置}</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python demo/image_demo.py demo/demo.jpg configs/yolo/yolov3_d53_mstrain-416_273e_coco.py checkpoints/yolov3_d53_mstrain-416_273e_coco-2b60fcd9.pth</span><br></pre></td></tr></table></figure><p>报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;demo/image_demo.py&quot;, line 4, in &lt;module&gt;</span><br><span class="line">    from mmdet.apis import (async_inference_detector, inference_detector,</span><br><span class="line">  File &quot;/home/glj/mmdetection-master/mmdet/apis/__init__.py&quot;, line 1, in &lt;module&gt;</span><br><span class="line">    from .inference import (async_inference_detector, inference_detector,</span><br><span class="line">  File &quot;/home/glj/mmdetection-master/mmdet/apis/inference.py&quot;, line 6, in &lt;module&gt;</span><br><span class="line">    from mmcv.ops import RoIPool</span><br><span class="line">  File &quot;/home/glj/anaconda3/lib/python3.7/site-packages/mmcv/ops/__init__.py&quot;, line 1, in &lt;module&gt;</span><br><span class="line">    from .bbox import bbox_overlaps</span><br><span class="line">  File &quot;/home/glj/anaconda3/lib/python3.7/site-packages/mmcv/ops/bbox.py&quot;, line 3, in &lt;module&gt;</span><br><span class="line">    ext_module = ext_loader.load_ext(&apos;_ext&apos;, [&apos;bbox_overlaps&apos;])</span><br><span class="line">  File &quot;/home/glj/anaconda3/lib/python3.7/site-packages/mmcv/utils/ext_loader.py&quot;, line 12, in load_ext</span><br><span class="line">    ext = importlib.import_module(&apos;mmcv.&apos; + name)</span><br><span class="line">  File &quot;/home/glj/anaconda3/lib/python3.7/importlib/__init__.py&quot;, line 127, in import_module</span><br><span class="line">    return _bootstrap._gcd_import(name[level:], package, level)</span><br><span class="line">ModuleNotFoundError: No module named &apos;mmcv._ext&apos;</span><br></pre></td></tr></table></figure><p>缺少了模块’mmcv._ext’</p><h2 id="排错"><a href="#排错" class="headerlink" title="排错"></a>排错</h2><p>上网查了之后是缺少了一个库mmcv-full</p><p>可以直接用以下方法安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install mmcv-full==&#123;version&#125;+&#123;pytorch&#125;+&#123;cuda&#125; -f https://download.openmmlab.com/mmcv/dist/index.html</span><br></pre></td></tr></table></figure><p>其中version指mmcv-full的版本号，可以直接使用latest下载最新，pytorch和cuda都代表版本号，比如我的环境是torch1.7.0，cuda是10.2就输入如下命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install mmcv-full==latest+torch1.7.0+cu102 -f https://download.openmmlab.com/mmcv/dist/index.html</span><br></pre></td></tr></table></figure><p>安装完成后再运行继续报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;demo/image_demo.py&quot;, line 3, in &lt;module&gt;</span><br><span class="line">    from mmdet.apis import inference_detector, init_detector, show_result_pyplot</span><br><span class="line">  File &quot;/home/glj/mmdetection-master/mmdet/__init__.py&quot;, line 25, in &lt;module&gt;</span><br><span class="line">    f&apos;MMCV==&#123;mmcv.__version__&#125; is used but incompatible. &apos; \</span><br><span class="line">AssertionError: MMCV==1.3.5 is used but incompatible. Please install mmcv&gt;=1.3.8, &lt;=1.4.0.</span><br></pre></td></tr></table></figure><p>说mmcv版本太低，我看了一下这个mmcv版本和mmcv-full版本似乎不一样</p><p><img src="/2021/08/05/mmdetection配置/1.png" alt="1"></p><p>cuda10.2+torch1.7似乎没有1.3.8以上的版本，又把mmcv版本降下来也不行，考虑到可能是和mmdetection的版本比匹配，上网查了一下果然：</p><p><img src="/2021/08/05/mmdetection配置/2.png" alt="2"></p><p>删除mmdet安装2.12.0再次运行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python demo/image_demo.py demo/demo.jpg configs/yolo/yolov3_d53_mstrain-416_273e_coco.py checkpoints/yolov3_d53_mstrain-416_273e_coco-2b60fcd9.pth</span><br></pre></td></tr></table></figure><p>得到结果：</p><p><img src="/2021/08/05/mmdetection配置/3.png" alt="3"></p><p>赢了</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;水一篇在linux上搭建mmdetection的博客&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="目标检测" scheme="http://yoursite.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="mmdetection" scheme="http://yoursite.com/tags/mmdetection/"/>
    
  </entry>
  
  <entry>
    <title>毕设：飞鸟图片识别</title>
    <link href="http://yoursite.com/2021/01/20/%E6%AF%95%E8%AE%BE%EF%BC%9A%E9%A3%9E%E9%B8%9F%E5%9B%BE%E7%89%87%E8%AF%86%E5%88%AB/"/>
    <id>http://yoursite.com/2021/01/20/%E6%AF%95%E8%AE%BE%EF%BC%9A%E9%A3%9E%E9%B8%9F%E5%9B%BE%E7%89%87%E8%AF%86%E5%88%AB/</id>
    <published>2021-01-20T02:24:02.000Z</published>
    <updated>2021-08-05T02:47:21.825Z</updated>
    
    <content type="html"><![CDATA[<h2 id="毕设：飞鸟图片识别"><a href="#毕设：飞鸟图片识别" class="headerlink" title="毕设：飞鸟图片识别"></a>毕设：飞鸟图片识别</h2><p>主要运用YOLO,CNN,Flask搭建神经网络识别飞鸟图片种类的服务<br><a id="more"></a></p><h2 id="1月20日"><a href="#1月20日" class="headerlink" title="1月20日"></a>1月20日</h2><h1 id="YOLOv3的学习"><a href="#YOLOv3的学习" class="headerlink" title="YOLOv3的学习"></a>YOLOv3的学习</h1><p>YOLO的作用主要是识别图片中的物体位置并进行分类，YOLOv3主要通过将图片划分为多个网格，然后定位物体在网格中的范围，其中结果又52<em>52，26</em>26,13<em>13的网格，分别对应小物体，中等物体和大物体<br><img src="/2021/01/20/毕设：飞鸟图片识别/2.png" alt="YOLOv3的图片识别"><br>首先了解神经网络结构，利用了DarkNet53结构，其结构主要如下<br>(图源：<a href="https://blog.csdn.net/weixin_44791964/article/details/105310627?ops_request_misc=%25257B%252522request%25255Fid%252522%25253A%252522161110983216780265465116%252522%25252C%252522scm%252522%25253A%25252220140713.130102334.pc%25255Fblog.%252522%25257D&amp;request_id=161110983216780265465116&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_v2~rank_v29-2-105310627.pc_v2_rank_blog_default&amp;utm_term=YOLO" target="_blank" rel="noopener">https://blog.csdn.net/weixin_44791964/article/details/105310627?ops_request_misc=%25257B%252522request%25255Fid%252522%25253A%252522161110983216780265465116%252522%25252C%252522scm%252522%25253A%25252220140713.130102334.pc%25255Fblog.%252522%25257D&amp;request_id=161110983216780265465116&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_v2~rank_v29-2-105310627.pc_v2_rank_blog_default&amp;utm_term=YOLO</a>)<br><img src="/2021/01/20/毕设：飞鸟图片识别/1.jpg" alt="DarkNet53"><br>个人经验：在学习一个新的神经网络的时候应该把其内部结构看做一个黑盒，首先了解其输出结果的含义。<br>YOLOv3结果是3,3,75的结果，其中75可分解为3</em>(20+1+4)<br>其中3代表三种物体类型，即识别大物体，中物体和小物体<br>20为物体种类(鸡鸭鱼人什么的)<br>1为该位置是否有物体的置信度<br>4代表具体位置(x，y坐标，长和宽)<br>DarkNet53的主要特点就是加入了残差网络(Residual block)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;个人理解：传统的神经网络为了追求更高的准确率往往会增加网络深度，这就可能导致了梯度爆炸的问题，解决方法就是加入正则化层(Batch Normalization)，但是在增加深度以后并没有获得更好的准确率，随后引入了残差网络，具体原理还没明白，后期实操以后如果会了再写，这里先把度娘百科贴上<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;深度残差网络。如果深层网络的后面那些层是恒等映射，那么模型就退化为一个浅层网络。那当前要解决的就是学习恒等映射函数了。 但是直接让一些层去拟合一个潜在的恒等映射函数 ，比较困难，这可能就是深层网络难以训练的原因。但是，如果把网络设计为 ,如图1。我们可以转换为学习一个残差函数 。 只要 ，就构成了一个恒等映射 。 而且，拟合残差肯定更加容易。</p><h2 id="1月21日"><a href="#1月21日" class="headerlink" title="1月21日"></a>1月21日</h2><p>尝试把DarkNet网络结构的代码连编带抄写出来，并自己对网络结构写上注释<br>完整DarkNet结构<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">class BasicBlock(nn.Module):</span><br><span class="line">    def __init__(self, planes):</span><br><span class="line">        super(BasicBlock, self).__init__()</span><br><span class="line">        </span><br><span class="line">        #残差网络层的网络结构定义</span><br><span class="line">        self.conv1 = nn.Conv2d(planes[1], planes[0], kernel_size=1,</span><br><span class="line">                               stride=1, padding=0, bias=False)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(planes[0])</span><br><span class="line">        self.relu1 = nn.LeakyReLU(0.1)</span><br><span class="line">        </span><br><span class="line">        self.conv2 = nn.Conv2d(planes[0], planes[1], kernel_size=3,</span><br><span class="line">                               stride=1, padding=1, bias=False)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(planes[1])</span><br><span class="line">        self.relu2 = nn.LeakyReLU(0.1)</span><br><span class="line">    </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        #残差边</span><br><span class="line">        residual = x</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu1(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu2(out)</span><br><span class="line"></span><br><span class="line">        #将进行了俩次卷积的结果与残差边相加</span><br><span class="line">        out = out + residual</span><br><span class="line">        return out</span><br><span class="line"></span><br><span class="line">class DarkNet(nn.Module):</span><br><span class="line">    #--------------------------------------------------------------------</span><br><span class="line">    #     传入layers为一个数组，表示每经过一个残差网络</span><br><span class="line">    #     堆积的层数</span><br><span class="line">    #--------------------------------------------------------------------</span><br><span class="line">    def __init__(self, layers):</span><br><span class="line">        super(DarkNet, self).__init__()</span><br><span class="line">        self.input_size = 32</span><br><span class="line">        #进行一次下采样418*418*3-&gt;418*418*32</span><br><span class="line">        self.conv1 = nn.Conv2d(3, self.input_size, kernel_size=3, stride=1, padding=1, bias=False)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(self.input_size)</span><br><span class="line">        self.relu1 = nn.LeakyReLU(0.1)</span><br><span class="line">        </span><br><span class="line">        #以下为残差网络</span><br><span class="line">        self.layer1 = self.make_layers([32, 64], layers[0])</span><br><span class="line">        self.layer2 = self.make_layers([64, 128], layers[1])</span><br><span class="line">        self.layer3 = self.make_layers([128, 256], layers[2])</span><br><span class="line">        self.layer4 = self.make_layers([256, 512], layers[3])</span><br><span class="line">        self.layer5 = self.make_layers([512, 1024], layers[4])</span><br><span class="line">        </span><br><span class="line">        self.layers_out_filters = [64, 128, 256, 512, 1024]</span><br><span class="line">        </span><br><span class="line">    #-------------------------------------------------------------------</span><br><span class="line">    #    planes为一个列表，表示输入的通道数和输出的通道数</span><br><span class="line">    #    blocks表示堆积的残差网络的层数</span><br><span class="line">    #-------------------------------------------------------------------</span><br><span class="line">    def make_layers(self, planes, blocks):</span><br><span class="line">        layers = []</span><br><span class="line">        </span><br><span class="line">        #首先下采样</span><br><span class="line">        layers.append([&quot;ds_conv&quot;, nn.Conv2d(planes[0], planes[1], kernel_size=3, stride=2, padding=1, bias=False)])</span><br><span class="line">        layers.append([&quot;ds_bn&quot;, nn.BatchNorm2d(planes[1])])</span><br><span class="line">        layers.append([&quot;ds_relu&quot;, nn.LeakyReLU(0.1)])</span><br><span class="line">        </span><br><span class="line">        for i in range(blocks):</span><br><span class="line">            layers.append([&quot;residual_&#123;&#125;&quot;.format(i+1), BasicBlock(planes)])</span><br><span class="line">        return nn.Sequential(OrderedDict(layers))</span><br><span class="line">        </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.relu1(x)</span><br><span class="line">        </span><br><span class="line">        #残差网络</span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        out1 = self.layer3(x)</span><br><span class="line">        out2 = self.layer4(x)</span><br><span class="line">        out3 = self.layer5(x)</span><br><span class="line">        </span><br><span class="line">        return out1, out2, out3</span><br></pre></td></tr></table></figure></p><h2 id="1月24日"><a href="#1月24日" class="headerlink" title="1月24日"></a>1月24日</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;今天没整啥有用的，纯把网络结构写完了，并且传入例子可以跑出结果了，附上代码吧<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">def conv2d(filter_in, filter_out, kernel_size):</span><br><span class="line">    pad = (kernel_size - 1) // 2 if kernel_size else 0</span><br><span class="line">    return nn.Sequential(OrderedDict([</span><br><span class="line">        (&quot;conv&quot;, nn.Conv2d(filter_in, filter_out, kernel_size=kernel_size, stride=1, padding=pad, bias=False)),</span><br><span class="line">        (&quot;bn&quot;, nn.BatchNorm2d(filter_out)),</span><br><span class="line">        (&quot;relu&quot;, nn.LeakyReLU(0.1)),</span><br><span class="line">    ]))</span><br><span class="line"></span><br><span class="line">def make_last_layers(filters_list, in_filters, out_filter):</span><br><span class="line">    m = nn.ModuleList([</span><br><span class="line">        conv2d(in_filters, filters_list[0], 1),</span><br><span class="line">        conv2d(filters_list[0], filters_list[1], 3),</span><br><span class="line">        conv2d(filters_list[1], filters_list[0], 1),</span><br><span class="line">        conv2d(filters_list[0], filters_list[1], 3),</span><br><span class="line">        conv2d(filters_list[1], filters_list[0], 1),</span><br><span class="line">        conv2d(filters_list[0], filters_list[1], 3),</span><br><span class="line">        nn.Conv2d(filters_list[1], out_filter, kernel_size=1,</span><br><span class="line">                                        stride=1, padding=0, bias=True)</span><br><span class="line">    ])</span><br><span class="line">    return m</span><br><span class="line"></span><br><span class="line">class YoloBody(nn.Module):</span><br><span class="line">    def __init__(self, config):</span><br><span class="line">        super(YoloBody, self).__init__()</span><br><span class="line">        self.config = config</span><br><span class="line">        </span><br><span class="line">        #----------------------------------------------------------</span><br><span class="line">        #       获得DarkNet网络</span><br><span class="line">        #----------------------------------------------------------</span><br><span class="line">        self.DarkNet = darknet53(None)</span><br><span class="line">        </span><br><span class="line">        #获得每一层DarkNet的输出通道数</span><br><span class="line">        out_filters = self.DarkNet.layers_out_filters</span><br><span class="line">        </span><br><span class="line">        #---------------------------------------------------------</span><br><span class="line">        #      计算最后输出的通道数</span><br><span class="line">        #      4+1+classes</span><br><span class="line">        #---------------------------------------------------------</span><br><span class="line">        final_out_filter1 = len(self.config[&quot;yolo&quot;][&quot;anchors&quot;][0]) * (5 + self.config[&quot;yolo&quot;][&quot;classes&quot;])</span><br><span class="line">        self.last_layer1 = make_last_layers([512,1024], out_filters[-1], final_out_filter1)</span><br><span class="line">        </span><br><span class="line">        final_out_filter2 = len(self.config[&quot;yolo&quot;][&quot;anchors&quot;][1]) * (5 + self.config[&quot;yolo&quot;][&quot;classes&quot;])</span><br><span class="line">        self.last_layer2_conv = conv2d(512, 256, 1)</span><br><span class="line">        self.last_layer2_upsample = nn.Upsample(scale_factor=2, mode=&apos;nearest&apos;)</span><br><span class="line">        self.last_layer2 = make_last_layers([256,512], 768, final_out_filter2)</span><br><span class="line">        </span><br><span class="line">        final_out_filter3 = len(self.config[&quot;yolo&quot;][&quot;anchors&quot;][2]) * (5 + self.config[&quot;yolo&quot;][&quot;classes&quot;])</span><br><span class="line">        self.last_layer3_conv = conv2d(256, 128, 1)</span><br><span class="line">        self.last_layer3_upsample = nn.Upsample(scale_factor=2, mode=&apos;nearest&apos;)</span><br><span class="line">        self.last_layer3 = make_last_layers([128,256], 384, final_out_filter3)</span><br><span class="line">        </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        #-----------------------------------------------------</span><br><span class="line">        #       将输出分成俩个，一个是结果</span><br><span class="line">        #       一个进行上采样</span><br><span class="line">        #-----------------------------------------------------</span><br><span class="line">        def _branch(last_layer, layer_in):</span><br><span class="line">            for i, e in enumerate(last_layer):</span><br><span class="line">                layer_in = e(layer_in)</span><br><span class="line">                if i == 4:</span><br><span class="line">                    out_branch = layer_in</span><br><span class="line">            return layer_in, out_branch</span><br><span class="line">            </span><br><span class="line">        x3, x2, x1 = self.DarkNet(x)</span><br><span class="line">            </span><br><span class="line">        #------------------------------------------------------</span><br><span class="line">        #         第一个特征层(大物体)</span><br><span class="line">        #         输出为13*13*1024</span><br><span class="line">        #------------------------------------------------------</span><br><span class="line">        out1, out1_branch = _branch(self.last_layer1, x1)</span><br><span class="line">            </span><br><span class="line">        x2_in = self.last_layer2_conv(out1_branch)</span><br><span class="line">        x2_in = self.last_layer2_upsample(x2_in)</span><br><span class="line">        </span><br><span class="line">        x2_in = torch.cat([x2_in, x2], 1)</span><br><span class="line">        </span><br><span class="line">        out2, out2_branch = _branch(self.last_layer2, x2_in)</span><br><span class="line">        </span><br><span class="line">        x3_in = self.last_layer3_conv(out2_branch)</span><br><span class="line">        x3_in = self.last_layer2_upsample(x3_in)</span><br><span class="line">        </span><br><span class="line">        x3_in = torch.cat([x3_in, x3], 1)</span><br><span class="line">        </span><br><span class="line">        out3, _ = _branch(self.last_layer3, x3_in)</span><br><span class="line">        return out1, out2, out3</span><br></pre></td></tr></table></figure></p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在原本darknet的基础上进行五层卷积，将三个输出下采样得到最终结果13<em>13</em>75，26<em>26</em>75，52<em>52</em>75</p><h2 id="1月28日"><a href="#1月28日" class="headerlink" title="1月28日"></a>1月28日</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在大体掌握网络结构以后，提出以下构想：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.因为样本图片中大部分只有一只鸟类为目标物体，决定取消掉原本YOLOv3结果中的三个先验框，也就是将结果的通道数缩小为25。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.将最终检测出的物体的先验框范围取出并进行resize后传入CNN网络进行第二次分类，并在得到结果后与YOLO的分类结果进行模型融合。</p><h2 id="1月29日"><a href="#1月29日" class="headerlink" title="1月29日"></a>1月29日</h2><h1 id="VOC数据集以及xml的读取"><a href="#VOC数据集以及xml的读取" class="headerlink" title="VOC数据集以及xml的读取"></a>VOC数据集以及xml的读取</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VOC数据集包括Annotations文件夹和JPEGImages文件夹：<br>JPEGImages文件夹存放图片<br>Annotations文件夹为每个图片配置一个xml文件，负责存储图片中的物体的位置以及bonding box的位置<br>如图所示<br><img src="/2021/01/20/毕设：飞鸟图片识别/3.jpg" alt="图片"><br>其对应xml如下，每有一个物体便有一个object标签，其中包括name和bonding box的四个角的坐标，这些属性都将作为训练中的target<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">&lt;annotation&gt;</span><br><span class="line">&lt;folder&gt;VOC2007&lt;/folder&gt;</span><br><span class="line">&lt;filename&gt;000005.jpg&lt;/filename&gt;</span><br><span class="line">&lt;source&gt;</span><br><span class="line">&lt;database&gt;The VOC2007 Database&lt;/database&gt;</span><br><span class="line">&lt;annotation&gt;PASCAL VOC2007&lt;/annotation&gt;</span><br><span class="line">&lt;image&gt;flickr&lt;/image&gt;</span><br><span class="line">&lt;flickrid&gt;325991873&lt;/flickrid&gt;</span><br><span class="line">&lt;/source&gt;</span><br><span class="line">&lt;owner&gt;</span><br><span class="line">&lt;flickrid&gt;archintent louisville&lt;/flickrid&gt;</span><br><span class="line">&lt;name&gt;?&lt;/name&gt;</span><br><span class="line">&lt;/owner&gt;</span><br><span class="line">&lt;size&gt;</span><br><span class="line">&lt;width&gt;500&lt;/width&gt;</span><br><span class="line">&lt;height&gt;375&lt;/height&gt;</span><br><span class="line">&lt;depth&gt;3&lt;/depth&gt;</span><br><span class="line">&lt;/size&gt;</span><br><span class="line">&lt;segmented&gt;0&lt;/segmented&gt;</span><br><span class="line">&lt;object&gt;</span><br><span class="line">&lt;name&gt;chair&lt;/name&gt;</span><br><span class="line">&lt;pose&gt;Rear&lt;/pose&gt;</span><br><span class="line">&lt;truncated&gt;0&lt;/truncated&gt;</span><br><span class="line">&lt;difficult&gt;0&lt;/difficult&gt;</span><br><span class="line">&lt;bndbox&gt;</span><br><span class="line">&lt;xmin&gt;263&lt;/xmin&gt;</span><br><span class="line">&lt;ymin&gt;211&lt;/ymin&gt;</span><br><span class="line">&lt;xmax&gt;324&lt;/xmax&gt;</span><br><span class="line">&lt;ymax&gt;339&lt;/ymax&gt;</span><br><span class="line">&lt;/bndbox&gt;</span><br><span class="line">&lt;/object&gt;</span><br><span class="line">&lt;object&gt;</span><br><span class="line">&lt;name&gt;chair&lt;/name&gt;</span><br><span class="line">&lt;pose&gt;Unspecified&lt;/pose&gt;</span><br><span class="line">&lt;truncated&gt;0&lt;/truncated&gt;</span><br><span class="line">&lt;difficult&gt;0&lt;/difficult&gt;</span><br><span class="line">&lt;bndbox&gt;</span><br><span class="line">&lt;xmin&gt;165&lt;/xmin&gt;</span><br><span class="line">&lt;ymin&gt;264&lt;/ymin&gt;</span><br><span class="line">&lt;xmax&gt;253&lt;/xmax&gt;</span><br><span class="line">&lt;ymax&gt;372&lt;/ymax&gt;</span><br><span class="line">&lt;/bndbox&gt;</span><br><span class="line">&lt;/object&gt;</span><br><span class="line">&lt;object&gt;</span><br><span class="line">&lt;name&gt;chair&lt;/name&gt;</span><br><span class="line">&lt;pose&gt;Unspecified&lt;/pose&gt;</span><br><span class="line">&lt;truncated&gt;1&lt;/truncated&gt;</span><br><span class="line">&lt;difficult&gt;1&lt;/difficult&gt;</span><br><span class="line">&lt;bndbox&gt;</span><br><span class="line">&lt;xmin&gt;5&lt;/xmin&gt;</span><br><span class="line">&lt;ymin&gt;244&lt;/ymin&gt;</span><br><span class="line">&lt;xmax&gt;67&lt;/xmax&gt;</span><br><span class="line">&lt;ymax&gt;374&lt;/ymax&gt;</span><br><span class="line">&lt;/bndbox&gt;</span><br><span class="line">&lt;/object&gt;</span><br><span class="line">&lt;object&gt;</span><br><span class="line">&lt;name&gt;chair&lt;/name&gt;</span><br><span class="line">&lt;pose&gt;Unspecified&lt;/pose&gt;</span><br><span class="line">&lt;truncated&gt;0&lt;/truncated&gt;</span><br><span class="line">&lt;difficult&gt;0&lt;/difficult&gt;</span><br><span class="line">&lt;bndbox&gt;</span><br><span class="line">&lt;xmin&gt;241&lt;/xmin&gt;</span><br><span class="line">&lt;ymin&gt;194&lt;/ymin&gt;</span><br><span class="line">&lt;xmax&gt;295&lt;/xmax&gt;</span><br><span class="line">&lt;ymax&gt;299&lt;/ymax&gt;</span><br><span class="line">&lt;/bndbox&gt;</span><br><span class="line">&lt;/object&gt;</span><br><span class="line">&lt;object&gt;</span><br><span class="line">&lt;name&gt;chair&lt;/name&gt;</span><br><span class="line">&lt;pose&gt;Unspecified&lt;/pose&gt;</span><br><span class="line">&lt;truncated&gt;1&lt;/truncated&gt;</span><br><span class="line">&lt;difficult&gt;1&lt;/difficult&gt;</span><br><span class="line">&lt;bndbox&gt;</span><br><span class="line">&lt;xmin&gt;277&lt;/xmin&gt;</span><br><span class="line">&lt;ymin&gt;186&lt;/ymin&gt;</span><br><span class="line">&lt;xmax&gt;312&lt;/xmax&gt;</span><br><span class="line">&lt;ymax&gt;220&lt;/ymax&gt;</span><br><span class="line">&lt;/bndbox&gt;</span><br><span class="line">&lt;/object&gt;</span><br><span class="line">&lt;/annotation&gt;</span><br></pre></td></tr></table></figure></p><h2 id="不好意思写日期了"><a href="#不好意思写日期了" class="headerlink" title="不好意思写日期了"></a>不好意思写日期了</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于长时间没有进展，也一直没写博客，现在基本做完了，虽然开始构思很多但后来发现差不多得了。。。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;先展示一下最终效果吧。<br><img src="/2021/01/20/毕设：飞鸟图片识别/4.png" alt="图片"></p><h1 id="在以下总结毕设中的经验与收获吧"><a href="#在以下总结毕设中的经验与收获吧" class="headerlink" title="在以下总结毕设中的经验与收获吧"></a>在以下总结毕设中的经验与收获吧</h1><p>1.关于先验框（anchors）的作用<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;YOLOv3作为最经典的one-stage识别算法之一，以Darknet53作为骨干网络拥有强大的特征提取能力，已经有足够的识别能力，但是在目标定位方面单纯的<br>以回归的方式去预测位置参数效果并不是很好，YOLOv1没有使用先验框，直接使特征矩阵通过全连接层预测结果，发现对于小物体的识别能力很差。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;YOLOv3在三种大小的特征矩阵中各加入三个先验框，配置如下图<br><img src="/2021/01/20/毕设：飞鸟图片识别/5.png" alt="图片"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过先验框的方式不同物体识别的效率更好，可是适应不同大小和形状的物体。<br>2.关于残差边<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在人们传统的思路中，卷积深度越多，特征提取越多，网络效果更好，但是后来实验证明在深度越多以后反而网络的识别效果会下降，发生网络退化的现<br>象，随后人们设计了残差网络的结构。<br><img src="/2021/01/20/毕设：飞鸟图片识别/6.png" alt="图片"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们知道，随着卷积深度的增加，特征提取更多，但包含的图像信息可能越来越少，将结果与卷积前的矩阵相加可以保证得到结果的图像信息至少大于输<br>入时的图像信息量，有效避免了网络退化的问题。<br>3.关于冻结训练<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;开始直接将代码写完en冲的时候，没过多久损失就达到了几千亿，这是很典型的梯度爆炸的现象，由于Darknet卷积深度太多，一个很小的梯度就可能经<br>过n层的反向传播成指数型增长，在前几次训练时冻结backbone，不更新其权重，在后面再完全解冻就可以很好地解决这个问题。<br>4.关于迁移学习<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;简单的50次epoch后虽然损失函数的结果已经很低了但是依然无法检测到物体，但是还是不想减小阈值，后来发现在原项目中读取了用imagenet训练的预<br>训练权重，在读取以后训练同样50次后结果便以很显著。随后尝试原项目中不读取预训练权重，同样经过50个epoch得到的结果依然很差，再次证明迁移学习方法的可行性与高效。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;毕设：飞鸟图片识别&quot;&gt;&lt;a href=&quot;#毕设：飞鸟图片识别&quot; class=&quot;headerlink&quot; title=&quot;毕设：飞鸟图片识别&quot;&gt;&lt;/a&gt;毕设：飞鸟图片识别&lt;/h2&gt;&lt;p&gt;主要运用YOLO,CNN,Flask搭建神经网络识别飞鸟图片种类的服务&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="YOLO" scheme="http://yoursite.com/tags/YOLO/"/>
    
      <category term="CNN" scheme="http://yoursite.com/tags/CNN/"/>
    
      <category term="Flask" scheme="http://yoursite.com/tags/Flask/"/>
    
  </entry>
  
  <entry>
    <title>腾讯安全竞赛机器学习</title>
    <link href="http://yoursite.com/2020/04/04/%E8%85%BE%E8%AE%AF%E5%AE%89%E5%85%A8%E7%AB%9E%E8%B5%9B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2020/04/04/%E8%85%BE%E8%AE%AF%E5%AE%89%E5%85%A8%E7%AB%9E%E8%B5%9B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</id>
    <published>2020-04-04T04:18:19.000Z</published>
    <updated>2020-04-04T04:43:42.234Z</updated>
    
    <content type="html"><![CDATA[<h2 id="腾讯安全竞赛机器学习"><a href="#腾讯安全竞赛机器学习" class="headerlink" title="腾讯安全竞赛机器学习"></a>腾讯安全竞赛机器学习</h2><a id="more"></a><p>昨天的腾讯安全竞赛着实给我人整晕了，还是太缺少这种比赛的经验，从中文十一点整到晚上十一点半勉强算完成，中间除了吃饭和看了会儿LPL也没怎么玩。</p><h1 id="1-开始"><a href="#1-开始" class="headerlink" title="1.开始"></a>1.开始</h1><p>题目直接截图了<br><img src="/2020/04/04/腾讯安全竞赛机器学习/0.png" alt="题目"><br>数据主要分为以下几个<br><img src="/2020/04/04/腾讯安全竞赛机器学习/数据目录结构.png" alt="数据目录结构"><br><img src="/2020/04/04/腾讯安全竞赛机器学习/1.png" alt="数据"><br><img src="/2020/04/04/腾讯安全竞赛机器学习/2.png" alt="数据"><br><img src="/2020/04/04/腾讯安全竞赛机器学习/3.png" alt="数据"><br><img src="/2020/04/04/腾讯安全竞赛机器学习/4.png" alt="数据"></p><h1 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2.数据预处理"></a>2.数据预处理</h1><p>数据比较分散，，先考虑将每个数据合并在统一的一个表格中，提取其中的有价值信息汇总为如下信息<br><img src="/2020/04/04/腾讯安全竞赛机器学习/5.png" alt="处理后的数据"><br>根据所有的openid处理数据，先将数据按照openid进行grouped by分组（如果不进行group by直接取值的话处理这几万条数据至少要一个半小时）<br>处理每个openid代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">def get_data(openid,label):</span><br><span class="line">ser = []</span><br><span class="line">uin = openid</span><br><span class="line">ser.append(uin)</span><br><span class="line">login_all_data = data_role_login.get_group(uin)</span><br><span class="line">logout_all_data = data_role_logout.get_group(uin)</span><br><span class="line">try:</span><br><span class="line">create_all_data = data_role_create.get_group(uin)</span><br><span class="line">except:</span><br><span class="line">create_all_data = [0]</span><br><span class="line">try:</span><br><span class="line">ser.append(int(login_all_data[&quot;platid&quot;].mode()))</span><br><span class="line">except:</span><br><span class="line">ser.append(int(login_all_data[&quot;platid&quot;].max()))</span><br><span class="line">try:</span><br><span class="line">ser.append(int(login_all_data[&quot;areaid&quot;].mode()))</span><br><span class="line">except:</span><br><span class="line">ser.append(int(login_all_data[&quot;areaid&quot;].max()))</span><br><span class="line">try:</span><br><span class="line">ser.append(int(login_all_data[&quot;worldid&quot;].mode()))</span><br><span class="line">except:</span><br><span class="line">ser.append(int(login_all_data[&quot;worldid&quot;].max()))</span><br><span class="line">ser.append(len(set(login_all_data[&quot;roleid&quot;])))</span><br><span class="line">ser.append(login_all_data[&quot;job&quot;].max())</span><br><span class="line">ser.append(len(login_all_data))</span><br><span class="line">levelup_all = 0</span><br><span class="line">for i in range(len(logout_all_data)):</span><br><span class="line">login = list(login_all_data[&quot;rolelevel&quot;])</span><br><span class="line">logout = list(logout_all_data[&quot;rolelevel&quot;])</span><br><span class="line">levelup_all = logout[i] - login[i]</span><br><span class="line">ser.append(int(levelup_all/len(login_all_data)))</span><br><span class="line">powerup_all = 0</span><br><span class="line">for i in range(len(logout_all_data)):</span><br><span class="line">login = list(login_all_data[&quot;power&quot;])</span><br><span class="line">logout = list(logout_all_data[&quot;power&quot;])</span><br><span class="line">powerup_all = logout[i] - login[i]</span><br><span class="line">ser.append(int(powerup_all/len(login_all_data)))</span><br><span class="line">friendup_all = 0</span><br><span class="line">for i in range(len(logout_all_data)):</span><br><span class="line">login = list(login_all_data[&quot;friendsnum&quot;])</span><br><span class="line">logout = list(logout_all_data[&quot;friendsnum&quot;])</span><br><span class="line">friendup_all = logout[i] - login[i]</span><br><span class="line">ser.append(int(friendup_all/len(login_all_data)))</span><br><span class="line">ser.append(int(logout_all_data[&quot;onlinetime&quot;].mean()/len(login_all_data)))</span><br><span class="line">ser.append(len(create_all_data))</span><br><span class="line">chat_cnt = data_uin_chat[data_uin_chat[&quot;uin&quot;] == uin][&quot;chat_cnt&quot;].values</span><br><span class="line">if len(chat_cnt) == 0:</span><br><span class="line">ser.append(0)</span><br><span class="line">else:</span><br><span class="line">ser.append(chat_cnt[0])</span><br><span class="line">ser.append(label)</span><br><span class="line">return ser</span><br></pre></td></tr></table></figure></p><p>部分数据缺少下线时的数据，一开始我补全了相关缺失值，但是使预测结果稍有降低，于是决定不考虑缺失下线时数据的用户以免影响模型质量。</p><p>处理完成后差不多有40000多条数据，把他们保存为CSV表格<br><img src="/2020/04/04/腾讯安全竞赛机器学习/6.png" alt="数据"></p><h1 id="3-决策树建模"><a href="#3-决策树建模" class="headerlink" title="3.决策树建模"></a>3.决策树建模</h1><p>按8:2分割训练集与测试集，直接扔进决策树，最后得到结果正确率九十多，召回率八十多<br><img src="/2020/04/04/腾讯安全竞赛机器学习/7.png" alt="决策树"></p><p>然后按照同样方法处理5号的数据，但这次缺失下线数据的相关项目进行补全</p><p><img src="/2020/04/04/腾讯安全竞赛机器学习/8.png" alt="数据"></p><p>最后带回模型得到结果</p><p><img src="/2020/04/04/腾讯安全竞赛机器学习/9.png" alt="结果"></p><h1 id="折腾了一天，进不进得去决赛的呗，爱咋咋滴吧"><a href="#折腾了一天，进不进得去决赛的呗，爱咋咋滴吧" class="headerlink" title="折腾了一天，进不进得去决赛的呗，爱咋咋滴吧"></a>折腾了一天，进不进得去决赛的呗，爱咋咋滴吧</h1>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;腾讯安全竞赛机器学习&quot;&gt;&lt;a href=&quot;#腾讯安全竞赛机器学习&quot; class=&quot;headerlink&quot; title=&quot;腾讯安全竞赛机器学习&quot;&gt;&lt;/a&gt;腾讯安全竞赛机器学习&lt;/h2&gt;
    
    </summary>
    
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="腾讯安全" scheme="http://yoursite.com/tags/%E8%85%BE%E8%AE%AF%E5%AE%89%E5%85%A8/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>LSTM写诗</title>
    <link href="http://yoursite.com/2020/02/02/LSTM%E5%86%99%E8%AF%97/"/>
    <id>http://yoursite.com/2020/02/02/LSTM%E5%86%99%E8%AF%97/</id>
    <published>2020-02-02T07:01:30.000Z</published>
    <updated>2020-02-02T08:53:29.861Z</updated>
    
    <content type="html"><![CDATA[<h2 id="利用lstm制作的写诗模型"><a href="#利用lstm制作的写诗模型" class="headerlink" title="利用lstm制作的写诗模型"></a>利用lstm制作的写诗模型</h2><a id="more"></a><h2 id="拜某只蝙蝠所赐，这几天在家通过多方面学习排错写好了LSTM作诗模型"><a href="#拜某只蝙蝠所赐，这几天在家通过多方面学习排错写好了LSTM作诗模型" class="headerlink" title="拜某只蝙蝠所赐，这几天在家通过多方面学习排错写好了LSTM作诗模型"></a>拜某只蝙蝠所赐，这几天在家通过多方面学习排错写好了LSTM作诗模型</h2><h1 id="github网址：https-github-com-AAAAAimer-LSTM-poems"><a href="#github网址：https-github-com-AAAAAimer-LSTM-poems" class="headerlink" title="github网址：https://github.com/AAAAAimer/LSTM-poems"></a>github网址：<a href="https://github.com/AAAAAimer/LSTM-poems" target="_blank" rel="noopener">https://github.com/AAAAAimer/LSTM-poems</a></h1><h1 id="1-爬取某网站的七言诗句共10891条作为训练数据"><a href="#1-爬取某网站的七言诗句共10891条作为训练数据" class="headerlink" title="1.爬取某网站的七言诗句共10891条作为训练数据"></a>1.爬取某网站的七言诗句共10891条作为训练数据</h1><p><img src="/2020/02/02/LSTM写诗/诗词网站.png" alt="诗词网站"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from lxml import etree</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">urls = []</span><br><span class="line">headers = &#123;</span><br><span class="line">&quot;User-Agent&quot;:&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:72.0) Gecko/20100101 Firefox/72.0&quot;</span><br><span class="line">&#125;</span><br><span class="line">for id in range(100):</span><br><span class="line">print(id)</span><br><span class="line">for i in range(1,50):</span><br><span class="line">try:</span><br><span class="line">if i==1:</span><br><span class="line">new_url = &quot;http://www.shicimingju.com/chaxun/zuozhe/&quot; + str(id) + &quot;.html&quot;</span><br><span class="line">else:</span><br><span class="line">new_url = &quot;http://www.shicimingju.com/chaxun/zuozhe/&quot; + str(id) + &quot;_&quot; + str(i) + &quot;.html&quot;</span><br><span class="line">r = requests.get(new_url,headers=headers)</span><br><span class="line">for j in range(2,40,2):</span><br><span class="line">html = etree.HTML(r.text)</span><br><span class="line">index = html.xpath(&quot;/html/body/div[4]/div[1]/div[1]/div[&quot; + str(j) + &quot;]/div[2]/h3/a/@href&quot;)</span><br><span class="line">urls.append(index)</span><br><span class="line">except:</span><br><span class="line">pass</span><br><span class="line"></span><br><span class="line">poems = &quot;&quot;</span><br><span class="line">for i in urls:</span><br><span class="line">try:</span><br><span class="line">url = &quot;http://www.shicimingju.com&quot; + i[0]</span><br><span class="line">r = requests.get(url,headers=headers)</span><br><span class="line">soup = BeautifulSoup(r.text)</span><br><span class="line">poem = soup.find(&quot;div&quot;,attrs=&#123;&quot;class&quot;:&quot;item_content&quot;&#125;)</span><br><span class="line">for j in poem.contents:</span><br><span class="line">if len(j) == 16 and str(j)[7] == &apos;，&apos; and str(j)[0] != &apos;\n&apos; and str(j)[1] != &apos;\n&apos;:</span><br><span class="line">poems = poems + str(j) + &quot;\n&quot;</span><br><span class="line">print(str(j))</span><br><span class="line">except:</span><br><span class="line">pass</span><br><span class="line"></span><br><span class="line">with open(r&quot;data\poems_res.txt&quot;,&quot;a+&quot;,encoding=&quot;UTF-8&quot;) as f:</span><br><span class="line">f.write(poems)</span><br></pre></td></tr></table></figure><p><img src="/2020/02/02/LSTM写诗/1.png" alt="爬下来的诗词"><br>后面将以此文本作为训练集进行训练</p><h1 id="2-文本读取以及数据预处理"><a href="#2-文本读取以及数据预处理" class="headerlink" title="2.文本读取以及数据预处理"></a>2.文本读取以及数据预处理</h1><p>以每行为单位，给每行诗词得开头与结尾加上标记字符’B’和’E’</p><p>众所周知，在万物皆可用数字的形式表示，在图片处理中，我们将图片以矩阵的方式传入神经网络，而在文本处理中，我们要做的是将训练集中所有的字符赋予一个编码值<br>比如：“床前明月光，”，‘床’是0，‘前’是1…‘，’是5，而回车符也要占有一位编码，我们将文字数字化以后就可以将它传入神经网络进行训练了。</p><p>(1)定义读取并且编码的load_txt函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">start_mark = &apos;B&apos;</span><br><span class="line">end_mark = &apos;E&apos;</span><br><span class="line"></span><br><span class="line">def load_txt(file_name):</span><br><span class="line">res = &#123;&#125;</span><br><span class="line">txt = []</span><br><span class="line">with open(file_name, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:</span><br><span class="line">for line in f.readlines():</span><br><span class="line">try:</span><br><span class="line">content = start_mark + line + end_mark</span><br><span class="line">txt.append(content)</span><br><span class="line">except:</span><br><span class="line">pass</span><br><span class="line"></span><br><span class="line">all_vocab = [word for line in txt for word in line]</span><br><span class="line">vocabs = list(set(all_vocab))</span><br><span class="line">vocabs.append(&quot; &quot;)</span><br><span class="line">vocabs = list(set(vocabs))</span><br><span class="line">vocabs = sorted(vocabs)</span><br><span class="line"></span><br><span class="line">count = len(vocabs)</span><br><span class="line">int_to_word = dict(zip(range(count), vocabs))</span><br><span class="line">word_to_int = dict(zip(vocabs, range(count)))</span><br><span class="line"></span><br><span class="line">encoded = [list(map(lambda word: word_to_int.get(word, count), char)) for char in txt]</span><br><span class="line"></span><br><span class="line">res[&quot;encoded&quot;] = encoded</span><br><span class="line">res[&quot;all_vocab&quot;] = all_vocab</span><br><span class="line">res[&quot;vocabs&quot;] = vocabs</span><br><span class="line">res[&quot;count&quot;] = count</span><br><span class="line">res[&quot;int_to_word&quot;] = int_to_word</span><br><span class="line">res[&quot;word_to_int&quot;] = word_to_int</span><br><span class="line">return res</span><br></pre></td></tr></table></figure><p>结果以字典的形式返回，其中包括<br>encoded：编码后的文本内容<br>all_vocab：所有字符的列表(有重复)<br>vocabs：所有字符的列表(无重复)<br>count：字符总数<br>int_to_word：编码转字符字典<br>word_to_int：字符转编码字典</p><h3 id="划重点，影响了我好多时间的问题，在生成所有字符列表时切忌使用集合，因为集合是无序的，在生成文本时再读取文本编码会和之前不一样，保险起见set之后转回列表并排序"><a href="#划重点，影响了我好多时间的问题，在生成所有字符列表时切忌使用集合，因为集合是无序的，在生成文本时再读取文本编码会和之前不一样，保险起见set之后转回列表并排序" class="headerlink" title="划重点，影响了我好多时间的问题，在生成所有字符列表时切忌使用集合，因为集合是无序的，在生成文本时再读取文本编码会和之前不一样，保险起见set之后转回列表并排序"></a>划重点，影响了我好多时间的问题，在生成所有字符列表时切忌使用集合，因为集合是无序的，在生成文本时再读取文本编码会和之前不一样，保险起见set之后转回列表并排序</h3><p>(2)定义获得训练batch的函数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def get_batches(batch_size, word_vec):</span><br><span class="line">x_batches = []</span><br><span class="line">y_batches = []</span><br><span class="line">n = len(word_vec)//batch_size</span><br><span class="line">for i in range(n):</span><br><span class="line">b_index = i * batch_size</span><br><span class="line">e_index = b_index + batch_size</span><br><span class="line"></span><br><span class="line">batch = word_vec[b_index:e_index]</span><br><span class="line">max_len = max(map(len,word_vec))</span><br><span class="line">x_data = np.full((batch_size, max_len), 1, np.int32)</span><br><span class="line">for j in range(batch_size):</span><br><span class="line">for k in range(len(batch[j])):</span><br><span class="line">x_data[j][k] = batch[j][k]</span><br><span class="line">y_data = np.copy(x_data)</span><br><span class="line">y_data[:, :-1] = x_data[:, 1:]</span><br><span class="line"></span><br><span class="line">x_batches.append(x_data)</span><br><span class="line">y_batches.append(y_data)</span><br><span class="line">return x_batches, y_batches</span><br></pre></td></tr></table></figure></p><h1 id="3-模型搭建"><a href="#3-模型搭建" class="headerlink" title="3.模型搭建"></a>3.模型搭建</h1><p>模型主要分为<br>输入层-&gt;LSTM层-&gt;全连接层-&gt;输出层<br>将模型封装为类<br>首先在构造函数时将神经网络的所有参数传入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class lstm_model:</span><br><span class="line"></span><br><span class="line">def __init__(self, batch_size, lstm_size, num_layers, learning_rate, num_classes):</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">初始化对象是传入神经网络参数</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">self.batch_size = batch_size</span><br><span class="line">self.lstm_size = lstm_size</span><br><span class="line">self.num_layers = num_layers</span><br><span class="line">self.learning_rate = learning_rate</span><br><span class="line">self.num_classes = num_classes</span><br></pre></td></tr></table></figure></p><p>定义gen_model函数生成网络结构<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">def gen_model(self, training=True):</span><br><span class="line"></span><br><span class="line">    self.x_input = tf.placeholder(tf.int32, [self.batch_size, None])</span><br><span class="line">    #如果不是训练，标签输入层设为None</span><br><span class="line">    if training:</span><br><span class="line">        self.y_input = tf.placeholder(tf.int32, [self.batch_size, None])</span><br><span class="line">    else:</span><br><span class="line">        self.y_input = None</span><br><span class="line"></span><br><span class="line">    #定义LSTM层</span><br><span class="line">    cell = tf.contrib.rnn.BasicLSTMCell(self.lstm_size, state_is_tuple=True)</span><br><span class="line">    lstm = tf.contrib.rnn.MultiRNNCell([cell] * self.num_layers, state_is_tuple=True)</span><br><span class="line"></span><br><span class="line">    if training:</span><br><span class="line">        self.initial_state = lstm.zero_state(batch_size=self.batch_size,dtype=tf.float32)</span><br><span class="line">    else:</span><br><span class="line">        self.initial_state = lstm.zero_state(batch_size=self.batch_size, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    embedding = tf.get_variable(&apos;embedding&apos;, initializer=tf.random_uniform([self.num_classes + 1, self.lstm_size], -1.0, 1.0))</span><br><span class="line">    inputs = tf.nn.embedding_lookup(embedding, self.x_input)</span><br><span class="line"></span><br><span class="line">    lstm_outputs, self.final_state = tf.nn.dynamic_rnn(lstm, inputs, initial_state=self.initial_state)</span><br><span class="line">    lstm_outputs = tf.reshape(lstm_outputs, [-1, self.lstm_size])</span><br><span class="line"></span><br><span class="line">    full_weight = tf.Variable(tf.truncated_normal([self.lstm_size, self.num_classes + 1]))</span><br><span class="line">    full_bias = tf.Variable(tf.zeros(shape=[self.num_classes + 1]))</span><br><span class="line">    </span><br><span class="line">    self.logits = tf.nn.bias_add(tf.matmul(lstm_outputs, full_weight), bias=full_bias)</span><br><span class="line"></span><br><span class="line">    if training:</span><br><span class="line">        labels = tf.one_hot(tf.reshape(self.y_input, [-1]), depth=self.num_classes + 1)</span><br><span class="line"></span><br><span class="line">        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=self.logits))</span><br><span class="line"></span><br><span class="line">        self.train_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)</span><br><span class="line"></span><br><span class="line">    else:</span><br><span class="line">        self.pred = tf.nn.softmax(self.logits)</span><br></pre></td></tr></table></figure></p><p>对于较小的文本数据可以对输入直接选择进行独热编码，但对于字符较多的数据会增大神经网络运算量， 此处选择embedding对输入数据编码<br><img src="/2020/02/02/LSTM写诗/2.png" alt="embedding"></p><p>具体的网络结构不做过多解释，就是在原本循环神经网络的基础上加入一条state支线防止梯度爆炸和梯度消失的问题，反正LSTM层的具体结构咱也不明白</p><h1 id="4-训练"><a href="#4-训练" class="headerlink" title="4.训练"></a>4.训练</h1><p>废话不多说，开始训练<br><img src="/2020/02/02/LSTM写诗/3.png" alt="参数设定"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">txt_res = load_txt(txt_path)</span><br><span class="line">x_batches, y_batches = get_batches(batch_size, txt_res[&quot;encoded&quot;])</span><br><span class="line"></span><br><span class="line">model = lstm_model(batch_size=batch_size, lstm_size=lstm_size, num_layers=num_layers,</span><br><span class="line">learning_rate=learning_rate, num_classes=len(txt_res[&quot;vocabs&quot;]))</span><br><span class="line">model.gen_model(training=True)</span><br><span class="line"></span><br><span class="line">saver = tf.train.Saver(tf.global_variables())</span><br><span class="line">all_var = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">sess.run(all_var)</span><br><span class="line"></span><br><span class="line">print(&quot;开始训练&quot;)</span><br><span class="line"></span><br><span class="line">n = len(txt_res[&quot;encoded&quot;])//batch_size</span><br><span class="line">c = 0</span><br><span class="line">for e in range(epochs):</span><br><span class="line">for b in range(n):</span><br><span class="line">feed = &#123;model.x_input:x_batches[b],model.y_input:y_batches[b]&#125;</span><br><span class="line">loss, _, _ = sess.run([model.loss, model.final_state, model.train_optimizer],</span><br><span class="line">feed_dict=feed)</span><br><span class="line">print(&quot;epoch:%d batch:%d counter:%d loss:%.8f&quot; % (e,b,c,loss))</span><br><span class="line">c += 1</span><br><span class="line">if e % 9 == 0:</span><br><span class="line">saver.save(sess, os.path.join(checkpoint, model_prefix), global_step=e)</span><br></pre></td></tr></table></figure><h1 id="5-生成文本"><a href="#5-生成文本" class="headerlink" title="5.生成文本"></a>5.生成文本</h1><p>训练结束后就可以开始作诗了，原理很简单，每次传入一个字符和上次训练留下的state，把每个输出结果利用numpy的概率函数取出字符的索引当做编码找到相应的字符就可以了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">def get_word(pred, vocabs):</span><br><span class="line">pred = pred[0]       </span><br><span class="line">pred /= np.sum(pred)</span><br><span class="line">index = np.random.choice(np.arange(len(pred)), p=pred)</span><br><span class="line">return vocabs[index]</span><br><span class="line"></span><br><span class="line">start_mark = &apos;B&apos;</span><br><span class="line">end_mark = &apos;E&apos;</span><br><span class="line">checkpoint = &quot;checkpoint/&quot;</span><br><span class="line">txt_path = &quot;data/poems_res.txt&quot;</span><br><span class="line">batch_size = 1</span><br><span class="line">learning_rate = 0.01</span><br><span class="line">lstm_size = 128</span><br><span class="line">num_layers = 2</span><br><span class="line"></span><br><span class="line">txt_res = load_txt(txt_path)</span><br><span class="line"></span><br><span class="line">model = lstm_model(batch_size=batch_size, lstm_size=lstm_size,</span><br><span class="line">num_layers=num_layers, learning_rate=learning_rate, num_classes=len(txt_res[&quot;vocabs&quot;]))</span><br><span class="line">model.gen_model(training=False)</span><br><span class="line"></span><br><span class="line">saver = tf.train.Saver(tf.global_variables())</span><br><span class="line">all_var = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())</span><br><span class="line"></span><br><span class="line">first_word = input(&quot;输入第一个字：&quot;)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">sess.run(all_var)</span><br><span class="line">checkpoint = tf.train.latest_checkpoint(checkpoint)</span><br><span class="line">saver.restore(sess, checkpoint)</span><br><span class="line"></span><br><span class="line">x = np.array([[txt_res[&quot;word_to_int&quot;][start_mark]]])</span><br><span class="line"></span><br><span class="line">feed = &#123;model.x_input:x&#125;</span><br><span class="line">pred, last_state = sess.run([model.pred,model.final_state], feed_dict=feed)</span><br><span class="line">x = np.array([[txt_res[&quot;word_to_int&quot;][first_word]]])</span><br><span class="line">word = first_word</span><br><span class="line">txt = &quot;&quot;</span><br><span class="line"></span><br><span class="line">c = 1</span><br><span class="line">while word != end_mark:</span><br><span class="line">txt = txt + word</span><br><span class="line">if c &gt; 1000:</span><br><span class="line">break</span><br><span class="line">feed = &#123;model.x_input:x, model.initial_state:last_state&#125;</span><br><span class="line">pred, last_state = sess.run([model.pred,model.final_state], feed_dict=feed)</span><br><span class="line">word = get_word(pred, txt_res[&quot;vocabs&quot;])</span><br><span class="line">x = np.array([[txt_res[&quot;word_to_int&quot;][word]]])</span><br><span class="line">c += 1</span><br><span class="line"></span><br><span class="line">print(txt)</span><br></pre></td></tr></table></figure><p>随便选一个字，比如李白最喜欢的月字</p><p><img src="/2020/02/02/LSTM写诗/4.png" alt="结果展示"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;利用lstm制作的写诗模型&quot;&gt;&lt;a href=&quot;#利用lstm制作的写诗模型&quot; class=&quot;headerlink&quot; title=&quot;利用lstm制作的写诗模型&quot;&gt;&lt;/a&gt;利用lstm制作的写诗模型&lt;/h2&gt;
    
    </summary>
    
    
    
      <category term="LSTM" scheme="http://yoursite.com/tags/LSTM/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>GAN</title>
    <link href="http://yoursite.com/2019/11/07/GAN(fashion%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%BA%E4%BE%8B)/"/>
    <id>http://yoursite.com/2019/11/07/GAN(fashion%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%BA%E4%BE%8B)/</id>
    <published>2019-11-07T09:59:18.000Z</published>
    <updated>2019-11-08T12:56:59.388Z</updated>
    
    <content type="html"><![CDATA[<h2 id="以fashion数据集为例的GAN网络"><a href="#以fashion数据集为例的GAN网络" class="headerlink" title="以fashion数据集为例的GAN网络"></a>以fashion数据集为例的GAN网络</h2><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">from PIL import Image</span><br><span class="line">import scipy</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line">fashion = input_data.read_data_sets(r&quot;D:\mnsit\fashion&quot;,one_hot=True)#读取fashion数据集</span><br><span class="line"></span><br><span class="line">#def get_image_batch():</span><br><span class="line">#imgs = []</span><br><span class="line">#for i in range(50):</span><br><span class="line">#ran = random.randint(0,90)</span><br><span class="line">#img = Image.open(images[ran])</span><br><span class="line">#img = img.convert(&quot;L&quot;) </span><br><span class="line">#img = img.resize([64,64])</span><br><span class="line">#data = img.getdata()</span><br><span class="line">#data = np.array(data)</span><br><span class="line">#data = data/255</span><br><span class="line">#data = np.reshape(data,(4096,))</span><br><span class="line">#imgs.append(data)</span><br><span class="line">#return np.array(imgs)</span><br><span class="line"></span><br><span class="line">def xavier_init(size):#利用xavier初始化参数</span><br><span class="line">in_dim = size[0]</span><br><span class="line">xavier_stddev = 1. / tf.sqrt(in_dim / 2.)</span><br><span class="line">return tf.random_normal(shape=size, stddev=xavier_stddev)</span><br><span class="line"></span><br><span class="line">Z = tf.placeholder(tf.float32,shape=[None,100],name=&apos;Z&apos;)</span><br><span class="line"></span><br><span class="line">G_W1 = tf.Variable(xavier_init([100,256]),name=&apos;G_W1&apos;)</span><br><span class="line">G_b1 = tf.Variable(tf.zeros(shape=[256]),name=&apos;G_b1&apos;)</span><br><span class="line">G_W2 = tf.Variable(xavier_init([256,784]),name=&apos;G_W2&apos;)</span><br><span class="line">G_b2 = tf.Variable(tf.zeros(shape=[784]),name=&apos;G_b2&apos;)</span><br><span class="line"># G_W3 = tf.Variable(xavier_init([512,1024]),name=&apos;G_W3&apos;)</span><br><span class="line"># G_b3 = tf.Variable(tf.zeros(shape=[1024]),name=&apos;G_b3&apos;)</span><br><span class="line"># G_W4 = tf.Variable(xavier_init([1024,2048]),name=&apos;G_W4&apos;)</span><br><span class="line"># G_b4 = tf.Variable(tf.zeros(shape=[2048]),name=&apos;G_b4&apos;)</span><br><span class="line"># G_W5 = tf.Variable(xavier_init([2048,4096]),name=&apos;G_W5&apos;)</span><br><span class="line"># G_b5 = tf.Variable(tf.zeros(shape=[4096]),name=&apos;G_b5&apos;)</span><br><span class="line">theta_G = [G_W1,G_b1,G_W2,G_b2]</span><br><span class="line"></span><br><span class="line">def generator(z):</span><br><span class="line">G_h1 = tf.nn.relu(tf.matmul(z,G_W1) + G_b1)</span><br><span class="line">#     G_h2 = tf.nn.relu(tf.matmul(G_h1,G_W2) + G_b2)</span><br><span class="line">#     G_h3 = tf.nn.relu(tf.matmul(G_h2,G_W3) + G_b3)</span><br><span class="line">#     G_h4 = tf.nn.relu(tf.matmul(G_h3,G_W4) + G_b4)</span><br><span class="line">G_log_prob = tf.matmul(G_h1,G_W2) + G_b2</span><br><span class="line">G_prob = tf.nn.sigmoid(G_log_prob)</span><br><span class="line">   </span><br><span class="line">   return G_prob</span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32,shape=[None,784],name=&apos;X&apos;)</span><br><span class="line"></span><br><span class="line">D_W1 = tf.Variable(xavier_init([784,256]),name=&apos;D_W1&apos;)</span><br><span class="line">D_b1 = tf.Variable(tf.zeros(shape=[256]),name=&apos;D_b1&apos;)</span><br><span class="line">D_W2 = tf.Variable(xavier_init([256,1]),name=&apos;D_W2&apos;)</span><br><span class="line">D_b2 = tf.Variable(tf.zeros(shape=[1]),name=&apos;D_b2&apos;)</span><br><span class="line"># D_W3 = tf.Variable(xavier_init([1024,512]),name=&apos;D_W3&apos;)</span><br><span class="line"># D_b3 = tf.Variable(tf.zeros(shape=[512]),name=&apos;D_b3&apos;)</span><br><span class="line"># D_W4 = tf.Variable(xavier_init([512,256]),name=&apos;D_W4&apos;)</span><br><span class="line"># D_b4 = tf.Variable(tf.zeros(shape=[256]),name=&apos;D_b4&apos;)</span><br><span class="line"># D_W5 = tf.Variable(xavier_init([256,128]),name=&apos;D_W5&apos;)</span><br><span class="line"># D_b5 = tf.Variable(tf.zeros(shape=[128]),name=&apos;D_b5&apos;)</span><br><span class="line"># D_W6 = tf.Variable(xavier_init([128,1]),name=&apos;D_W6&apos;)</span><br><span class="line"># D_b6 = tf.Variable(tf.zeros(shape=[1]),name=&apos;D_b6&apos;)</span><br><span class="line">theta_D = [D_W1,D_b1,D_W2,D_b2]</span><br><span class="line"></span><br><span class="line">def descriminator(x):</span><br><span class="line">D_h1 = tf.nn.relu(tf.matmul(x,D_W1) + D_b1)</span><br><span class="line">#     D_h2 = tf.nn.relu(tf.matmul(D_h1,D_W2) + D_b2)</span><br><span class="line">#     D_h3 = tf.nn.relu(tf.matmul(D_h2,D_W3) + D_b3)</span><br><span class="line">#     D_h4 = tf.nn.relu(tf.matmul(D_h3,D_W4) + D_b4)</span><br><span class="line">#     D_h5 = tf.nn.relu(tf.matmul(D_h4,D_W5) + D_b5)</span><br><span class="line">D_log_prob = tf.matmul(D_h1, D_W2) + D_b2</span><br><span class="line">D_prob = tf.nn.sigmoid(D_log_prob)</span><br><span class="line">   </span><br><span class="line">   return D_prob, D_log_prob</span><br><span class="line"></span><br><span class="line">G_sample = generator(Z)</span><br><span class="line"></span><br><span class="line">D_real, D_log_real = descriminator(X)</span><br><span class="line">D_fake, D_log_fake = descriminator(G_sample)</span><br><span class="line"></span><br><span class="line">D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1. - D_fake))</span><br><span class="line">G_loss = -tf.reduce_mean(tf.log(D_fake))</span><br><span class="line"></span><br><span class="line">D_solver = tf.train.AdadeltaOptimizer(learning_rate=0.1).minimize(D_loss, var_list=theta_D)</span><br><span class="line">G_solver = tf.train.AdadeltaOptimizer(learning_rate=0.1).minimize(G_loss, var_list=theta_G)</span><br><span class="line"></span><br><span class="line">def sample_Z(m,n):</span><br><span class="line">return np.random.uniform(-1.,1.,size=[m,n])</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">def save_images(data,size,path):</span><br><span class="line">data = np.resize(data,[size[0],size[1]])</span><br><span class="line">data = data * 255</span><br><span class="line">new_im = Image.fromarray(data)</span><br><span class="line">new_im = new_im.convert(&apos;L&apos;)</span><br><span class="line">new_im.save(path)</span><br><span class="line"></span><br><span class="line">for i in range(60000):</span><br><span class="line">X_mb,_ = fashion.train.next_batch(50)</span><br><span class="line"></span><br><span class="line">_, D_loss_curr = sess.run([D_solver,D_loss], feed_dict=&#123;X:X_mb, Z:sample_Z(50,100)&#125;)</span><br><span class="line">_, G_loss_curr = sess.run([G_solver,G_loss], feed_dict=&#123;Z:sample_Z(50,100)&#125;)</span><br><span class="line">print(i)</span><br><span class="line">if i%1000==0 :</span><br><span class="line">print(&quot;经过第&quot; + str(i) + &quot;次:&quot;)</span><br><span class="line">print(&quot;识别器损失函数：&quot;)</span><br><span class="line">print(sess.run(D_loss, feed_dict=&#123;X:X_mb, Z:sample_Z(50,100)&#125;))</span><br><span class="line">print(&quot;生成器损失函数：&quot;)</span><br><span class="line">print(sess.run(G_loss, feed_dict=&#123;Z:sample_Z(50,100)&#125;))</span><br><span class="line"></span><br><span class="line">print()</span><br><span class="line">if i%1000 == 0:</span><br><span class="line">G_image = sess.run(G_sample,feed_dict=&#123;X:X_mb, Z:sample_Z(50,100)&#125;)</span><br><span class="line">print(i)</span><br><span class="line">print(G_image)</span><br><span class="line">save_images(G_image,[196,196],&quot;D://pic//&quot; + str(i) + &quot;.jpg&quot;)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;以fashion数据集为例的GAN网络&quot;&gt;&lt;a href=&quot;#以fashion数据集为例的GAN网络&quot; class=&quot;headerlink&quot; title=&quot;以fashion数据集为例的GAN网络&quot;&gt;&lt;/a&gt;以fashion数据集为例的GAN网络&lt;/h2&gt;
    
    </summary>
    
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>JAVA HashMap的排序</title>
    <link href="http://yoursite.com/2019/10/17/JAVA-HashMap%E7%9A%84%E6%8E%92%E5%BA%8F/"/>
    <id>http://yoursite.com/2019/10/17/JAVA-HashMap%E7%9A%84%E6%8E%92%E5%BA%8F/</id>
    <published>2019-10-17T10:26:28.000Z</published>
    <updated>2019-10-17T10:46:47.923Z</updated>
    
    <content type="html"><![CDATA[<h2 id="JAVA-HashMap的排序"><a href="#JAVA-HashMap的排序" class="headerlink" title="JAVA HashMap的排序"></a>JAVA HashMap的排序</h2><a id="more"></a><p>众所周知，字典是没有顺序的，所以我们在Java中应该如何给字典按值排序呢？</p><h1 id="以今天做的一道题为实例"><a href="#以今天做的一道题为实例" class="headerlink" title="以今天做的一道题为实例"></a>以今天做的一道题为实例</h1><p><img src="/2019/10/17/JAVA-HashMap的排序/1.png" alt="题目"><br><img src="/2019/10/17/JAVA-HashMap的排序/2.png" alt="题目"><br>首先将每个人与他对应的收入存到一个HashMap中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Map&lt;Integer, Float&gt; person = new HashMap&lt;Integer,Float&gt;();  //定义一个HashMap</span><br><span class="line"></span><br><span class="line">for(int i=0;i&lt;n;i++) &#123;</span><br><span class="line">person.put(i, (float) 0.0);//初始化</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">float sum = 0;//统计每个人发的钱</span><br><span class="line"></span><br><span class="line">for(int i=0;i&lt;n;i++) &#123;</span><br><span class="line">int m = in.nextInt();</span><br><span class="line"></span><br><span class="line">for(int j=0;j&lt;m;j++) &#123;</span><br><span class="line">int id = in.nextInt();//收钱人的编号</span><br><span class="line">float money = in.nextFloat()/100;//收钱数量</span><br><span class="line">float tmp = person.get(id-1);</span><br><span class="line">person.put(id-1,tmp+money);</span><br><span class="line">sum += money;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">float tmp = person.get(i);</span><br><span class="line">person.put(i, tmp-sum);</span><br><span class="line">sum = 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>下面引入Map.Entry的概念：<br>Map的entrySet()方法返回一个实现Map.Entry接口的对象集合。集合中每个对象都是底层Map中一个特定的键/值对。通过这个集合的迭代器，获得每一个条目(唯一获取方式)的键或值并对值进行更改。<br>我们只需要利用Map的entrySet方法就可以把HashMap类型的容器转换为Map.Entry的容器，随后把它放到一个列表里就可以排序了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Map.Entry&lt;Integer,Float&gt;&gt; mlist = new ArrayList&lt;&gt;();//定义装Map.Entry的容器</span><br><span class="line"></span><br><span class="line">for(Map.Entry&lt;Integer, Float&gt; m : person.entrySet()) &#123;</span><br><span class="line">mlist.add(m);//将Map.Entry放入容器</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">mlist.sort(new Comparator&lt;Map.Entry&lt;Integer, Float&gt;&gt;() &#123;//排序并重写排序方法</span><br><span class="line">@Override</span><br><span class="line">public int compare(Entry&lt;Integer, Float&gt; o1, Entry&lt;Integer, Float&gt; o2) &#123;</span><br><span class="line">// TODO Auto-generated method stub</span><br><span class="line">if(o1.getValue()==o2.getValue()) &#123;//在数值相同时，选择比较编号的大小</span><br><span class="line">return (o1.getKey().compareTo(o2.getKey()));</span><br><span class="line">&#125;else &#123;</span><br><span class="line">return -(o1.getValue().compareTo(o2.getValue()));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></p><p>最后输出排序后的结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for(int i=0;i&lt;n;i++) &#123;</span><br><span class="line">System.out.printf(&quot;%d %.2f&quot;,mlist.get(i).getKey()+1,mlist.get(i).getValue());</span><br><span class="line">System.out.println();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1 11.63</span><br><span class="line">2 3.63</span><br><span class="line">8 3.63</span><br><span class="line">3 2.11</span><br><span class="line">7 1.69</span><br><span class="line">6 -1.67</span><br><span class="line">9 -2.18</span><br><span class="line">10 -3.26</span><br><span class="line">5 -3.26</span><br><span class="line">4 -12.32</span><br></pre></td></tr></table></figure><p>完整代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">import java.math.RoundingMode;</span><br><span class="line">import java.text.NumberFormat;</span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.Comparator;</span><br><span class="line">import java.util.HashMap;</span><br><span class="line">import java.util.List;</span><br><span class="line">import java.util.Map;</span><br><span class="line">import java.util.Map.Entry;</span><br><span class="line">import java.util.Scanner;</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">Scanner in = new Scanner(System.in);</span><br><span class="line"></span><br><span class="line">int n = in.nextInt();</span><br><span class="line"></span><br><span class="line">Map&lt;Integer, Float&gt; person = new HashMap&lt;Integer,Float&gt;();</span><br><span class="line"></span><br><span class="line">for(int i=0;i&lt;n;i++) &#123;</span><br><span class="line">person.put(i, (float) 0.0);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">float sum = 0;</span><br><span class="line"></span><br><span class="line">for(int i=0;i&lt;n;i++) &#123;</span><br><span class="line">int m = in.nextInt();</span><br><span class="line"></span><br><span class="line">for(int j=0;j&lt;m;j++) &#123;</span><br><span class="line">int id = in.nextInt();</span><br><span class="line">float money = in.nextFloat()/100;</span><br><span class="line">float tmp = person.get(id-1);</span><br><span class="line">person.put(id-1,tmp+money);</span><br><span class="line">sum += money;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">float tmp = person.get(i);</span><br><span class="line">person.put(i, tmp-sum);</span><br><span class="line">sum = 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">List&lt;Map.Entry&lt;Integer,Float&gt;&gt; mlist = new ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">for(Map.Entry&lt;Integer, Float&gt; m : person.entrySet()) &#123;</span><br><span class="line">mlist.add(m);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">mlist.sort(new Comparator&lt;Map.Entry&lt;Integer, Float&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public int compare(Entry&lt;Integer, Float&gt; o1, Entry&lt;Integer, Float&gt; o2) &#123;</span><br><span class="line">// TODO Auto-generated method stub</span><br><span class="line">if(o1.getValue()==o2.getValue()) &#123;</span><br><span class="line">return (o1.getKey().compareTo(o2.getKey()));</span><br><span class="line">&#125;else &#123;</span><br><span class="line">return -(o1.getValue().compareTo(o2.getValue()));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">for(int i=0;i&lt;n;i++) &#123;</span><br><span class="line">System.out.printf(&quot;%d %.2f&quot;,mlist.get(i).getKey()+1,mlist.get(i).getValue());</span><br><span class="line">System.out.println();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;JAVA-HashMap的排序&quot;&gt;&lt;a href=&quot;#JAVA-HashMap的排序&quot; class=&quot;headerlink&quot; title=&quot;JAVA HashMap的排序&quot;&gt;&lt;/a&gt;JAVA HashMap的排序&lt;/h2&gt;
    
    </summary>
    
    
    
      <category term="JAVA" scheme="http://yoursite.com/tags/JAVA/"/>
    
      <category term="Hashmap" scheme="http://yoursite.com/tags/Hashmap/"/>
    
  </entry>
  
  <entry>
    <title>Python给爷爬京东评论</title>
    <link href="http://yoursite.com/2019/09/04/Python%E7%BB%99%E7%88%B7%E7%88%AC%E4%BA%AC%E4%B8%9C%E8%AF%84%E8%AE%BA/"/>
    <id>http://yoursite.com/2019/09/04/Python%E7%BB%99%E7%88%B7%E7%88%AC%E4%BA%AC%E4%B8%9C%E8%AF%84%E8%AE%BA/</id>
    <published>2019-09-04T08:07:50.000Z</published>
    <updated>2019-09-04T08:51:15.589Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Python爬取京东评论"><a href="#Python爬取京东评论" class="headerlink" title="Python爬取京东评论"></a>Python爬取京东评论</h2><a id="more"></a><p>利用Python 爪巴耳又 京东商品评论</p><h1 id="爪巴-前分析"><a href="#爪巴-前分析" class="headerlink" title="爪巴 前分析"></a>爪巴 前分析</h1><p>爷把爪牙伸向了买不起的Beats耳机，首先看一下评论区的源码<br><img src="/2019/09/04/Python给爷爬京东评论/0.png" alt="评论区的源码"><br>这里是一个动态获取的json字符串，如果直接用url爬的话会爬下来一堆加载中，所以需要进行动态的分析</p><p>谷歌自带的抓包功能可以很好的找到请求包，右键检查，然后选择network，直接搜索‘comment’找一找就有了<br><img src="/2019/09/04/Python给爷爬京东评论/1.png" alt="谷歌"><br>然后找到他的请求链接<br><img src="/2019/09/04/Python给爷爬京东评论/2.png" alt="url"><br>我们可以看到，这里request下来的是一个json字符串，这样好说了，连BeautifulSoup解析都用不找了<br>一开始我直接get的这个url爬下来的是这样的<br><img src="/2019/09/04/Python给爷爬京东评论/3.jpg" alt="直接 爪巴"><br>这也是京东的反爬虫措施吧，一定是请求头少了什么东西，后来我把cookie加上也没有用，最后直接把请求头里的所有东西都扔了进去。<br><img src="/2019/09/04/Python给爷爬京东评论/4.png" alt="请求头"><br><img src="/2019/09/04/Python给爷爬京东评论/5.png" alt="请求头"><br>最后爬下来的json字符串<br>里面评论的key值是’comment’<br><img src="/2019/09/04/Python给爷爬京东评论/6.png" alt="结果"><br>如图，开头还有别的字符(‘fetchJSON_comment98vv2115(‘)，用lstrip和rstrip把它去掉就可以了<br>另外url中page表示的评论的页数，于是用俩个循环嵌套就可以爬取多页的内容了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">for i in range(0,10):     #爬取10页的内容</span><br><span class="line">url = &apos;https://sclub.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv2115&amp;productId=7824797&amp;score=0&amp;sortType=5&amp;page=&apos; + str(i) + &apos;&amp;pageSize=10&amp;isShadowSku=0&amp;fold=1&apos;      #i是评论的页数</span><br><span class="line">r = requests.get(url, headers=headers)    #请求并传入headers的值</span><br><span class="line">text = json.loads(r.text.lstrip(&apos;fetchJSON_comment98vv2115(&apos;).rstrip(&apos;);&apos;))</span><br><span class="line">for i in text[&apos;comments&apos;]:</span><br><span class="line">print(b,&quot;:&quot;,i[&apos;content&apos;])</span><br><span class="line">b = b + 1      #定义一个b变量来做计数君</span><br><span class="line">time.sleep(2)     #每页爬取的时候中途停止2秒</span><br></pre></td></tr></table></figure></p><p>最终结果<br><img src="/2019/09/04/Python给爷爬京东评论/7.png" alt="结果"></p><h2 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">import json</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">&apos;authority&apos;: &apos;sclub.jd.com&apos;,</span><br><span class="line">&apos;method&apos;: &apos;GET&apos;,</span><br><span class="line">&apos;path&apos;: &apos;/comment/productPageComments.action?callback=fetchJSON_comment98vv2115&amp;productId=7824797&amp;score=0&amp;sortType=5&amp;page=0&amp;pageSize=10&amp;isShadowSku=0&amp;fold=1&apos;,</span><br><span class="line">&apos;scheme&apos;: &apos;https&apos;,</span><br><span class="line">&apos;accept&apos;: &apos;*/*&apos;,</span><br><span class="line">&apos;accept-encoding&apos;: &apos;gzip, deflate, br&apos;,</span><br><span class="line">&apos;accept-language&apos;: &apos;zh-CN,zh;q=0.8&apos;,</span><br><span class="line">&apos;cahce-control&apos;: &apos;no-cache&apos;,</span><br><span class="line">&apos;cookie&apos;: &apos;shshshfpa=724ff748-c09c-07a4-690b-c530bd4e234d-1530001150; areaId=13; ipLoc-djd=13-1112-46666-0; PCSYCityID=CN_370000_370900_370902; user-key=bcbe746b-bd12-459c-9cb8-045ad278ea79; unpl=V2_ZzNtbUtXEEYnDRVUKE1VVWILFl4RVERGcgBCVH0aVAJvBUIJclRCFX0UR1xnGFkUZwQZXUBcRxBFCEdkexhdBGcHGllBUnMldDhFVEsRbAFiBBZVQlNLF0U4QWRLGVkAbwEWX0ZSXxZwD0RceClaBmcAE1VBVUoldDhGV3gQWQRuAhJfchwtFDgMQ1N%2fEVwBbwEiXHJU; cn=0; _pst=jd_620cddcd73a7b; unick=jd_156620pmr; pin=jd_620cddcd73a7b; thor=30A9230ACAE426D9C78A61CA7837F8D4A77BFA4F3607E3425FD97285B18FFEDF7D372E2534A97BA24CE61F470A8CB4A7B6CE99A44EDEFBD8B8D4856643B149AB0FFB2E9577084D09FA8EA5545746BBDF53BBD14E299935AB79C8717385446A36E543845F9902FAF9E83E0A41A599115969A77C7556C9FFF82AFD004B093B81EED7196D38448FBA76BB93CD90ECA1DBB8A4DE49041B2D4B5313D1FEB6760B337B; _tp=B%2BRjxGpLYGiCYbbWQ3shnXaN1fmT5iioD2%2BLJab0QNI%3D; pinId=8MwzGUoDS8UZAU3ai6-NlLV9-x-f3wj7; __jdv=76161171|buy.jiegeng.com|t_1000159524_|tuiguang|81bbc4b1be8a4952b26b6951729697ae|1567584853053; shshshfp=e3e6a9aab1f87ae8815309aa3e941e39; shshshsID=48dcc30254839ca93b4bfeaa57d55b4f_7_1567584939419; shshshfpb=17d0f1d7f357e402fbd82bc9ae4377ed7a0e7b4804022131b5b31f6f8e; __jda=122270672.546591593.1549889886.1567513578.1567584839.11; __jdb=122270672.15.546591593|11.1567584839; __jdc=122270672; 3AB9D23F7A4B3C9B=YEQZE4K5VITVEWIDGLZYFPVYLSZMBCW3G4PE3V7T7VJGQXWUSM4S4CUXEI4IUGGORLSAJ2DFODWXLNZH75FBMPP54Q; __jdu=546591593&apos;,</span><br><span class="line">&apos;pragma&apos;: &apos;no-cache&apos;,</span><br><span class="line">&apos;referer&apos;: &apos;https://item.jd.com/7824797.html&apos;,</span><br><span class="line">&apos;user-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36&apos;,</span><br><span class="line">&#125;</span><br><span class="line">b = 1     #计数君</span><br><span class="line"></span><br><span class="line">for i in range(0,10):</span><br><span class="line">url = &apos;https://sclub.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv2115&amp;productId=7824797&amp;score=0&amp;sortType=5&amp;page=&apos; + str(i) + &apos;&amp;pageSize=10&amp;isShadowSku=0&amp;fold=1&apos;</span><br><span class="line">r = requests.get(url, headers=headers)    #请求并传入headers的值</span><br><span class="line">text = json.loads(r.text.lstrip(&apos;fetchJSON_comment98vv2115(&apos;).rstrip(&apos;);&apos;))</span><br><span class="line">for i in text[&apos;comments&apos;]:</span><br><span class="line">print(b,&quot;:&quot;,i[&apos;content&apos;])</span><br><span class="line">b = b + 1      #生成b来做一个计数君</span><br><span class="line">time.sleep(2)     #没页爬取的时候中途停止2秒</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Python爬取京东评论&quot;&gt;&lt;a href=&quot;#Python爬取京东评论&quot; class=&quot;headerlink&quot; title=&quot;Python爬取京东评论&quot;&gt;&lt;/a&gt;Python爬取京东评论&lt;/h2&gt;
    
    </summary>
    
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>wxml调用js的函数</title>
    <link href="http://yoursite.com/2019/05/20/wxml%E8%B0%83%E7%94%A8js%E7%9A%84%E5%87%BD%E6%95%B0/"/>
    <id>http://yoursite.com/2019/05/20/wxml%E8%B0%83%E7%94%A8js%E7%9A%84%E5%87%BD%E6%95%B0/</id>
    <published>2019-05-20T03:18:03.000Z</published>
    <updated>2019-05-20T03:48:37.195Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;这几天一直在整微信小程序，一直很头疼的是如何在wxml调用js里的有参函数，因为button标签里的bindtap属性直接使用函数名，<br><a id="more"></a><br>所以没法传参，但是js提供了，函数中会默认传入标签的参数，所以可以利用它来做文章。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">addfood: function (res) &#123;</span><br><span class="line">console.log(res)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;直接打印一下res参数查看一下<br><img src="/2019/05/20/wxml调用js的函数/1.png" alt="打印res"><br>大体浏览了一下我注意到了在target下面有个dataset参数<br><img src="/2019/05/20/wxml调用js的函数/2.png" alt="dataset"><br>由此可以理解，可以通过标签自带的一些属性来实现wxml对于js的参数传递，只是想办法寻找用什么来传递就好了。<br>于是我xjbd看了看button标签的属性，发现了data-属性<br><img src="/2019/05/20/wxml调用js的函数/3.png" alt="data-"><br>给data-赋值后，再打印res看到了结果中有一个空的键值对应了我刚刚写的值<br><img src="/2019/05/20/wxml调用js的函数/5.png" alt="data的值"><br><img src="/2019/05/20/wxml调用js的函数/4.png" alt="data的值"><br>于是机智的我光速明白了data-后面的”-“后是键，后面=跟上他的value值就好了<br>随后按图赋值<br><img src="/2019/05/20/wxml调用js的函数/6.png" alt="data的值"><br>再打印就可以看到res的dataset值<br><img src="/2019/05/20/wxml调用js的函数/7.png" alt="data的值"><br>然后在函数里通过res.target.dataset.键就可以得到这些数据了</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;这几天一直在整微信小程序，一直很头疼的是如何在wxml调用js里的有参函数，因为button标签里的bindtap属性直接使用函数名，&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="wxml" scheme="http://yoursite.com/tags/wxml/"/>
    
      <category term="js" scheme="http://yoursite.com/tags/js/"/>
    
  </entry>
  
  <entry>
    <title>看着整整神经网络</title>
    <link href="http://yoursite.com/2019/05/05/%E7%9C%8B%E7%9D%80%E6%95%B4%E6%95%B4%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2019/05/05/%E7%9C%8B%E7%9D%80%E6%95%B4%E6%95%B4%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</id>
    <published>2019-05-05T12:30:37.000Z</published>
    <updated>2019-05-09T09:30:21.621Z</updated>
    
    <content type="html"><![CDATA[<h1 id="初识神经网络"><a href="#初识神经网络" class="headerlink" title="初识神经网络"></a>初识神经网络</h1><p>这篇文章主要对神经网络进行一下简单的概述，毕竟如果把里面的每一个点拉出来都够讲好久<br><a id="more"></a><br>本文主要参考来源：<a href="https://blog.csdn.net/young2415/article/details/81772068" target="_blank" rel="noopener">https://blog.csdn.net/young2415/article/details/81772068</a></p><p>文章中实现了一个简单的二维输入和输出的神经网络<br><img src="/2019/05/05/看着整整神经网络/1.png" alt="示意图"></p><p>一个神经网络主要由输入层，隐藏层与输出层组成</p><h2 id="如何去理解？"><a href="#如何去理解？" class="headerlink" title="如何去理解？"></a>如何去理解？</h2><p>1.比如说我们看到了一只小狗（输入层获得了动物的基本信息，如毛发颜色）</p><p>2.随后我们的神经元进行处理（隐藏层将输入的图像信息处理为数字信息）</p><p>3.最后，我们得到结果，它99.9%是只狗，0.1%是只猫（输出层输出结果）</p><p>文中生成了二维的数据，最终被归为了俩类（利用红色与蓝色标记）<br><img src="/2019/05/05/看着整整神经网络/2.png" alt="数据"><br>即隐藏层通过处理输入层输入的二维数据得到了另一组二维数据（即每一种类型的概率）最后从输入层得到结果<br>数据生成代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 生成数据集并绘制出来</span><br><span class="line">np.random.seed(0)</span><br><span class="line">X, y = sklearn.datasets.make_moons(200, noise=0.20)</span><br><span class="line">plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)</span><br></pre></td></tr></table></figure></p><h1 id="神经网络如何实现预测"><a href="#神经网络如何实现预测" class="headerlink" title="神经网络如何实现预测"></a>神经网络如何实现预测</h1><p>神经网络预测的关键就在于隐藏层对于输入数据的处理，我们的理想结果是我们传入一个类型1的数据，最后他的计算结果是[1,0]（当然这是不可能的），类型2的结果就是[0,1]，这需要我们不断地去调整隐藏层中公式的参数让正确的类型的概率尽可能的高。<br>通俗的讲，我们把传入的数据通过特定的函数将其生成为一个概率值，即预测结果，一个二维数值，俩个数相加等于1，这个特定的函数称为激活函数</p><script type="math/tex; mode=display">z_1 = xW_1+b_1</script><script type="math/tex; mode=display">a_1 = tanh(z_1)</script><script type="math/tex; mode=display">z_2 = a_1W_2+b_2 $​$$$ a_2 = y = softmax(z_2)</script><p>z表示每一层的输入,a表示激活函数后的输出，至于他的参数各在那一层我也很迷惑，后来看了它每一层矩阵的维度才大体理解，此处的W参数并不是一个单个的数，而是一个矩阵向量，输入的数据与W1点乘改变为隐藏层的维度。<br>例如文中的20000个二维数据与W1（2x3的矩阵）相乘以后生成了一个20000x3的矩阵，然后就是通过隐藏层的激活函数将数据规范化，随后通过W2再将数据转换为与输出维度相同（即二维），最后就是使用特殊的激活函数（文中用的softmax）转换成了0-1的数值，即最后我们需要的的概率。<br>大体结构我手绘了一下…<br><img src="/2019/05/05/看着整整神经网络/3.jpg" alt="示意"></p><p>&emsp;&emsp;可能各位已经看迷糊了，这一统操作我们究竟在干什么？为什么我们这样就可以生成他的概率？<br>&emsp;&emsp;我个人认为这可能就是神经网络最难理解的地方吧。我们在隐藏层内的数据其实并没有任何意义的，我们需要的是根据最后输出的结果对比他的真实情况再去调整W和B的值来让最后的输出的正确类别的概率向1逼近。<br>&emsp;&emsp;也就是说我们没必要去纠结于隐藏层中数据的意义，因为他只是我们得到结果过程中的一个中介，即使它最后的结果荒唐离谱，我们也可以通过后期调参让它逼近正确的数值，这就是一个神经网络学习的过程。<br>&emsp;&emsp;至于W该如何选择它的维度（即选择隐藏层的维度），就需要自己去自己评判了，因为过大和过小都会使模型欠拟合或者过拟合。<br><img src="/2019/05/05/看着整整神经网络/4.png" alt="更换W的参数"><br>至于调参的原理，这就涉及到bp神经网络另一个重要的知识点</p><h1 id="神经网络调参：反向传播"><a href="#神经网络调参：反向传播" class="headerlink" title="神经网络调参：反向传播"></a>神经网络调参：反向传播</h1><p>&emsp;&emsp;前面说到，神经网络的学习过程就在于根据预测结果与正确值的差距进行调参的过程，那么如何进行这个过程呢，这就涉及到bp神经网络的重要内容，反向传播。<br>&emsp;&emsp;何为反向传播，就是我们将我们预测结果与真实值的偏差重新传入神经网络，只不过是从输出层反向传入，再根据得到的结果调节参数。<br>&emsp;&emsp;也就是说我们需要找到一个让参数调整的值，使得这个偏差最小，如何去寻找这个值呢，这里就要使用到梯度下降的原理了<br><img src="/2019/05/05/看着整整神经网络/5.jpg" alt="梯度下降"><br>&emsp;&emsp;很好理解，y轴为偏差，x轴为我们的参数的数值也就是W或B，我们想要让它向最低值逼近，只需要对它求导找到它的梯度（可以理解为就是当前点最大的斜率），然后就让参数以此斜率乘一个数进行加减，至于这个数，称为学习率，如果学习率小了，效率就会降低，太大了有可能超越最低点。<br>&emsp;&emsp;至于这个导数怎么求就涉及到高数的东西了，自己去了解吧，这里直接放原文中的公式吧。</p><script type="math/tex; mode=display">δ_3 = y(真实值)-y</script><script type="math/tex; mode=display">δ_2 = (1-tanh^2z_1)δ_3(W_2)^T</script><script type="math/tex; mode=display">\frac {∂L} {∂W_2} = (a_1)^Tδ_3</script><script type="math/tex; mode=display">\frac {∂L} {∂b_2} = δ_3</script><script type="math/tex; mode=display">\frac {∂L} {∂W_1} = x^Tδ_2</script><script type="math/tex; mode=display">\frac {∂L} {∂b_1} = δ_2</script><p>最后我们计算出的$\frac {∂L} {∂W_2}$，$\frac {∂L} {∂b_2}$，$\frac {∂L} {∂W_1}$，$\frac {∂L} {∂b_1}$再乘学习率就是我们需要参数的改变量了</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;初识神经网络&quot;&gt;&lt;a href=&quot;#初识神经网络&quot; class=&quot;headerlink&quot; title=&quot;初识神经网络&quot;&gt;&lt;/a&gt;初识神经网络&lt;/h1&gt;&lt;p&gt;这篇文章主要对神经网络进行一下简单的概述，毕竟如果把里面的每一个点拉出来都够讲好久&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="简要" scheme="http://yoursite.com/tags/%E7%AE%80%E8%A6%81/"/>
    
  </entry>
  
  <entry>
    <title>openpyxl</title>
    <link href="http://yoursite.com/2019/05/02/openpyxl/"/>
    <id>http://yoursite.com/2019/05/02/openpyxl/</id>
    <published>2019-05-02T14:46:27.000Z</published>
    <updated>2019-05-09T09:30:23.461Z</updated>
    
    <content type="html"><![CDATA[<h1 id="关于Pyhton将数据写入excel表"><a href="#关于Pyhton将数据写入excel表" class="headerlink" title="关于Pyhton将数据写入excel表"></a>关于Pyhton将数据写入excel表</h1><p>今天被强迫性建模，处理数据后又忘记了怎么把数据写入excel表格，现找到了之前的源码复制粘贴，还是在这里记录一下吧以后方便copy。<br><a id="more"></a></p><h2 id="openyxl"><a href="#openyxl" class="headerlink" title="openyxl"></a>openyxl</h2><p>之前找了半天才找到的比较好用的库，先直接上代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def WriteExcel(data, colIndex, sheet):</span><br><span class="line">wb = openpyxl.load_workbook(&quot;路径&quot;)</span><br><span class="line">ws = wb.get_sheet_by_name(sheet)</span><br><span class="line">for i in range(len(data)):</span><br><span class="line">ws.cell(row=i+1, column=colIndex).value = float(data[i])</span><br><span class="line">wb.save(&quot;路径&quot;)</span><br></pre></td></tr></table></figure></p><p>这里写的是把list写入excel的一列，wb.get_sheet_by_name(sheet)中参数是工作簿的名称。<br>cell(row=i+1, column=colIndex).value = float(data[i])中cell函数的参数就是行和列，直接更改他的value属性就可以了。不过要注意的是，row和col最小的取值是1，和平日的大部分索引不同。</p><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>openxyl只能读取xlsx格式的表格<br>错误提示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">InvalidFileException: openpyxl does not support the old .xls file format, please use xlrd to read this file, or convert it to the more recent .xlsx file format.</span><br></pre></td></tr></table></figure></p><h2 id="最后，祈求在我学会神经网络之前不要再让我建模了！！！"><a href="#最后，祈求在我学会神经网络之前不要再让我建模了！！！" class="headerlink" title="最后，祈求在我学会神经网络之前不要再让我建模了！！！"></a>最后，祈求在我学会神经网络之前不要再让我建模了！！！</h2>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;关于Pyhton将数据写入excel表&quot;&gt;&lt;a href=&quot;#关于Pyhton将数据写入excel表&quot; class=&quot;headerlink&quot; title=&quot;关于Pyhton将数据写入excel表&quot;&gt;&lt;/a&gt;关于Pyhton将数据写入excel表&lt;/h1&gt;&lt;p&gt;今天被强迫性建模，处理数据后又忘记了怎么把数据写入excel表格，现找到了之前的源码复制粘贴，还是在这里记录一下吧以后方便copy。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Pyhton" scheme="http://yoursite.com/tags/Pyhton/"/>
    
      <category term="openpyxl" scheme="http://yoursite.com/tags/openpyxl/"/>
    
      <category term="Big-Digital" scheme="http://yoursite.com/tags/Big-Digital/"/>
    
  </entry>
  
  <entry>
    <title>First Blog</title>
    <link href="http://yoursite.com/2019/04/29/First-Blog/"/>
    <id>http://yoursite.com/2019/04/29/First-Blog/</id>
    <published>2019-04-29T12:30:28.000Z</published>
    <updated>2020-08-05T15:28:53.293Z</updated>
    
    <content type="html"><![CDATA[<p>第一篇博客</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;第一篇博客&lt;/p&gt;

      
    
    </summary>
    
    
    
      <category term="First" scheme="http://yoursite.com/tags/First/"/>
    
      <category term="happy" scheme="http://yoursite.com/tags/happy/"/>
    
  </entry>
  
</feed>
