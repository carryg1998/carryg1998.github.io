<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-01-21T06:50:55.504Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>CarryG</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>毕设：飞鸟图片识别</title>
    <link href="http://yoursite.com/2021/01/20/%E6%AF%95%E8%AE%BE%EF%BC%9A%E9%A3%9E%E9%B8%9F%E5%9B%BE%E7%89%87%E8%AF%86%E5%88%AB/"/>
    <id>http://yoursite.com/2021/01/20/%E6%AF%95%E8%AE%BE%EF%BC%9A%E9%A3%9E%E9%B8%9F%E5%9B%BE%E7%89%87%E8%AF%86%E5%88%AB/</id>
    <published>2021-01-20T02:24:02.000Z</published>
    <updated>2021-01-21T06:50:55.504Z</updated>
    
    <content type="html"><![CDATA[<h2 id="毕设：飞鸟图片识别"><a href="#毕设：飞鸟图片识别" class="headerlink" title="毕设：飞鸟图片识别"></a>毕设：飞鸟图片识别</h2><p>主要运用YOLO,CNN,Flask搭建神经网络识别飞鸟图片种类的服务<br><a id="more"></a></p><h2 id="1月20日"><a href="#1月20日" class="headerlink" title="1月20日"></a>1月20日</h2><h1 id="YOLOv3的学习"><a href="#YOLOv3的学习" class="headerlink" title="YOLOv3的学习"></a>YOLOv3的学习</h1><p>YOLO的作用主要是识别图片中的物体位置并进行分类，YOLOv3主要通过将图片划分为多个网格，然后定位物体在网格中的范围，其中结果又52<em>52，26</em>26,13<em>13的网格，分别对应小物体，中等物体和大物体<br><img src="/2021/01/20/毕设：飞鸟图片识别/2.png" alt="YOLOv3的图片识别"><br>首先了解神经网络结构，利用了DarkNet53结构，其结构主要如下<br>(图源：<a href="https://blog.csdn.net/weixin_44791964/article/details/105310627?ops_request_misc=%25257B%252522request%25255Fid%252522%25253A%252522161110983216780265465116%252522%25252C%252522scm%252522%25253A%25252220140713.130102334.pc%25255Fblog.%252522%25257D&amp;request_id=161110983216780265465116&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_v2~rank_v29-2-105310627.pc_v2_rank_blog_default&amp;utm_term=YOLO" target="_blank" rel="noopener">https://blog.csdn.net/weixin_44791964/article/details/105310627?ops_request_misc=%25257B%252522request%25255Fid%252522%25253A%252522161110983216780265465116%252522%25252C%252522scm%252522%25253A%25252220140713.130102334.pc%25255Fblog.%252522%25257D&amp;request_id=161110983216780265465116&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_v2~rank_v29-2-105310627.pc_v2_rank_blog_default&amp;utm_term=YOLO</a>)<br><img src="/2021/01/20/毕设：飞鸟图片识别/1.jpg" alt="DarkNet53"><br>个人经验：在学习一个新的神经网络的时候应该把其内部结构看做一个黑盒，首先了解其输出结果的含义。<br>YOLOv3结果是3,3,75的结果，其中75可分解为3</em>(20+1+4)<br>其中3代表三种物体类型，即识别大物体，中物体和小物体<br>20为物体种类(鸡鸭鱼人什么的)<br>1为该位置是否有物体的置信度<br>4代表具体位置(x，y坐标，长和宽)<br>DarkNet53的主要特点就是加入了残差网络(Residual block)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;个人理解：传统的神经网络为了追求更高的准确率往往会增加网络深度，这就可能导致了梯度爆炸的问题，解决方法就是加入正则化层(Batch Normalization)，但是在增加深度以后并没有获得更好的准确率，随后引入了残差网络，具体原理还没明白，后期实操以后如果会了再写，这里先把度娘百科贴上<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;深度残差网络。如果深层网络的后面那些层是恒等映射，那么模型就退化为一个浅层网络。那当前要解决的就是学习恒等映射函数了。 但是直接让一些层去拟合一个潜在的恒等映射函数 ，比较困难，这可能就是深层网络难以训练的原因。但是，如果把网络设计为 ,如图1。我们可以转换为学习一个残差函数 。 只要 ，就构成了一个恒等映射 。 而且，拟合残差肯定更加容易。</p><h2 id="1月21日"><a href="#1月21日" class="headerlink" title="1月21日"></a>1月21日</h2><p>尝试把DarkNet网络结构的代码连编带抄写出来，并自己对网络结构写上注释<br>完整DarkNet结构<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">class BasicBlock(nn.Module):</span><br><span class="line">    def __init__(self, planes):</span><br><span class="line">        super(BasicBlock, self).__init__()</span><br><span class="line">        </span><br><span class="line">        #残差网络层的网络结构定义</span><br><span class="line">        self.conv1 = nn.Conv2d(planes[1], planes[0], kernel_size=1,</span><br><span class="line">                               stride=1, padding=0, bias=False)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(planes[0])</span><br><span class="line">        self.relu1 = nn.LeakyReLU(0.1)</span><br><span class="line">        </span><br><span class="line">        self.conv2 = nn.Conv2d(planes[0], planes[1], kernel_size=3,</span><br><span class="line">                               stride=1, padding=1, bias=False)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(planes[1])</span><br><span class="line">        self.relu2 = nn.LeakyReLU(0.1)</span><br><span class="line">    </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        #残差边</span><br><span class="line">        residual = x</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu1(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu2(out)</span><br><span class="line"></span><br><span class="line">        #将进行了俩次卷积的结果与残差边相加</span><br><span class="line">        out = out + residual</span><br><span class="line">        return out</span><br><span class="line"></span><br><span class="line">class DarkNet(nn.Module):</span><br><span class="line">    #--------------------------------------------------------------------</span><br><span class="line">    #     传入layers为一个数组，表示每经过一个残差网络</span><br><span class="line">    #     堆积的层数</span><br><span class="line">    #--------------------------------------------------------------------</span><br><span class="line">    def __init__(self, layers):</span><br><span class="line">        super(DarkNet, self).__init__()</span><br><span class="line">        self.input_size = 32</span><br><span class="line">        #进行一次下采样418*418*3-&gt;418*418*32</span><br><span class="line">        self.conv1 = nn.Conv2d(3, self.input_size, kernel_size=3, stride=1, padding=1, bias=False)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(self.input_size)</span><br><span class="line">        self.relu1 = nn.LeakyReLU(0.1)</span><br><span class="line">        </span><br><span class="line">        #以下为残差网络</span><br><span class="line">        self.layer1 = self.make_layers([32, 64], layers[0])</span><br><span class="line">        self.layer2 = self.make_layers([64, 128], layers[1])</span><br><span class="line">        self.layer3 = self.make_layers([128, 256], layers[2])</span><br><span class="line">        self.layer4 = self.make_layers([256, 512], layers[3])</span><br><span class="line">        self.layer5 = self.make_layers([512, 1024], layers[4])</span><br><span class="line">        </span><br><span class="line">        self.layers_out_filters = [64, 128, 256, 512, 1024]</span><br><span class="line">        </span><br><span class="line">    #-------------------------------------------------------------------</span><br><span class="line">    #    planes为一个列表，表示输入的通道数和输出的通道数</span><br><span class="line">    #    blocks表示堆积的残差网络的层数</span><br><span class="line">    #-------------------------------------------------------------------</span><br><span class="line">    def make_layers(self, planes, blocks):</span><br><span class="line">        layers = []</span><br><span class="line">        </span><br><span class="line">        #首先下采样</span><br><span class="line">        layers.append([&quot;ds_conv&quot;, nn.Conv2d(planes[0], planes[1], kernel_size=3, stride=2, padding=1, bias=False)])</span><br><span class="line">        layers.append([&quot;ds_bn&quot;, nn.BatchNorm2d(planes[1])])</span><br><span class="line">        layers.append([&quot;ds_relu&quot;, nn.LeakyReLU(0.1)])</span><br><span class="line">        </span><br><span class="line">        for i in range(blocks):</span><br><span class="line">            layers.append([&quot;residual_&#123;&#125;&quot;.format(i+1), BasicBlock(planes)])</span><br><span class="line">        return nn.Sequential(OrderedDict(layers))</span><br><span class="line">        </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.relu1(x)</span><br><span class="line">        </span><br><span class="line">        #残差网络</span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        out1 = self.layer3(x)</span><br><span class="line">        out2 = self.layer4(x)</span><br><span class="line">        out3 = self.layer5(x)</span><br><span class="line">        </span><br><span class="line">        return out1, out2, out3</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;毕设：飞鸟图片识别&quot;&gt;&lt;a href=&quot;#毕设：飞鸟图片识别&quot; class=&quot;headerlink&quot; title=&quot;毕设：飞鸟图片识别&quot;&gt;&lt;/a&gt;毕设：飞鸟图片识别&lt;/h2&gt;&lt;p&gt;主要运用YOLO,CNN,Flask搭建神经网络识别飞鸟图片种类的服务&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="YOLO" scheme="http://yoursite.com/tags/YOLO/"/>
    
      <category term="CNN" scheme="http://yoursite.com/tags/CNN/"/>
    
      <category term="Flask" scheme="http://yoursite.com/tags/Flask/"/>
    
  </entry>
  
  <entry>
    <title>腾讯安全竞赛机器学习</title>
    <link href="http://yoursite.com/2020/04/04/%E8%85%BE%E8%AE%AF%E5%AE%89%E5%85%A8%E7%AB%9E%E8%B5%9B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2020/04/04/%E8%85%BE%E8%AE%AF%E5%AE%89%E5%85%A8%E7%AB%9E%E8%B5%9B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</id>
    <published>2020-04-04T04:18:19.000Z</published>
    <updated>2020-04-04T04:43:42.234Z</updated>
    
    <content type="html"><![CDATA[<h2 id="腾讯安全竞赛机器学习"><a href="#腾讯安全竞赛机器学习" class="headerlink" title="腾讯安全竞赛机器学习"></a>腾讯安全竞赛机器学习</h2><a id="more"></a><p>昨天的腾讯安全竞赛着实给我人整晕了，还是太缺少这种比赛的经验，从中文十一点整到晚上十一点半勉强算完成，中间除了吃饭和看了会儿LPL也没怎么玩。</p><h1 id="1-开始"><a href="#1-开始" class="headerlink" title="1.开始"></a>1.开始</h1><p>题目直接截图了<br><img src="/2020/04/04/腾讯安全竞赛机器学习/0.png" alt="题目"><br>数据主要分为以下几个<br><img src="/2020/04/04/腾讯安全竞赛机器学习/数据目录结构.png" alt="数据目录结构"><br><img src="/2020/04/04/腾讯安全竞赛机器学习/1.png" alt="数据"><br><img src="/2020/04/04/腾讯安全竞赛机器学习/2.png" alt="数据"><br><img src="/2020/04/04/腾讯安全竞赛机器学习/3.png" alt="数据"><br><img src="/2020/04/04/腾讯安全竞赛机器学习/4.png" alt="数据"></p><h1 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2.数据预处理"></a>2.数据预处理</h1><p>数据比较分散，，先考虑将每个数据合并在统一的一个表格中，提取其中的有价值信息汇总为如下信息<br><img src="/2020/04/04/腾讯安全竞赛机器学习/5.png" alt="处理后的数据"><br>根据所有的openid处理数据，先将数据按照openid进行grouped by分组（如果不进行group by直接取值的话处理这几万条数据至少要一个半小时）<br>处理每个openid代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">def get_data(openid,label):</span><br><span class="line">ser = []</span><br><span class="line">uin = openid</span><br><span class="line">ser.append(uin)</span><br><span class="line">login_all_data = data_role_login.get_group(uin)</span><br><span class="line">logout_all_data = data_role_logout.get_group(uin)</span><br><span class="line">try:</span><br><span class="line">create_all_data = data_role_create.get_group(uin)</span><br><span class="line">except:</span><br><span class="line">create_all_data = [0]</span><br><span class="line">try:</span><br><span class="line">ser.append(int(login_all_data[&quot;platid&quot;].mode()))</span><br><span class="line">except:</span><br><span class="line">ser.append(int(login_all_data[&quot;platid&quot;].max()))</span><br><span class="line">try:</span><br><span class="line">ser.append(int(login_all_data[&quot;areaid&quot;].mode()))</span><br><span class="line">except:</span><br><span class="line">ser.append(int(login_all_data[&quot;areaid&quot;].max()))</span><br><span class="line">try:</span><br><span class="line">ser.append(int(login_all_data[&quot;worldid&quot;].mode()))</span><br><span class="line">except:</span><br><span class="line">ser.append(int(login_all_data[&quot;worldid&quot;].max()))</span><br><span class="line">ser.append(len(set(login_all_data[&quot;roleid&quot;])))</span><br><span class="line">ser.append(login_all_data[&quot;job&quot;].max())</span><br><span class="line">ser.append(len(login_all_data))</span><br><span class="line">levelup_all = 0</span><br><span class="line">for i in range(len(logout_all_data)):</span><br><span class="line">login = list(login_all_data[&quot;rolelevel&quot;])</span><br><span class="line">logout = list(logout_all_data[&quot;rolelevel&quot;])</span><br><span class="line">levelup_all = logout[i] - login[i]</span><br><span class="line">ser.append(int(levelup_all/len(login_all_data)))</span><br><span class="line">powerup_all = 0</span><br><span class="line">for i in range(len(logout_all_data)):</span><br><span class="line">login = list(login_all_data[&quot;power&quot;])</span><br><span class="line">logout = list(logout_all_data[&quot;power&quot;])</span><br><span class="line">powerup_all = logout[i] - login[i]</span><br><span class="line">ser.append(int(powerup_all/len(login_all_data)))</span><br><span class="line">friendup_all = 0</span><br><span class="line">for i in range(len(logout_all_data)):</span><br><span class="line">login = list(login_all_data[&quot;friendsnum&quot;])</span><br><span class="line">logout = list(logout_all_data[&quot;friendsnum&quot;])</span><br><span class="line">friendup_all = logout[i] - login[i]</span><br><span class="line">ser.append(int(friendup_all/len(login_all_data)))</span><br><span class="line">ser.append(int(logout_all_data[&quot;onlinetime&quot;].mean()/len(login_all_data)))</span><br><span class="line">ser.append(len(create_all_data))</span><br><span class="line">chat_cnt = data_uin_chat[data_uin_chat[&quot;uin&quot;] == uin][&quot;chat_cnt&quot;].values</span><br><span class="line">if len(chat_cnt) == 0:</span><br><span class="line">ser.append(0)</span><br><span class="line">else:</span><br><span class="line">ser.append(chat_cnt[0])</span><br><span class="line">ser.append(label)</span><br><span class="line">return ser</span><br></pre></td></tr></table></figure></p><p>部分数据缺少下线时的数据，一开始我补全了相关缺失值，但是使预测结果稍有降低，于是决定不考虑缺失下线时数据的用户以免影响模型质量。</p><p>处理完成后差不多有40000多条数据，把他们保存为CSV表格<br><img src="/2020/04/04/腾讯安全竞赛机器学习/6.png" alt="数据"></p><h1 id="3-决策树建模"><a href="#3-决策树建模" class="headerlink" title="3.决策树建模"></a>3.决策树建模</h1><p>按8:2分割训练集与测试集，直接扔进决策树，最后得到结果正确率九十多，召回率八十多<br><img src="/2020/04/04/腾讯安全竞赛机器学习/7.png" alt="决策树"></p><p>然后按照同样方法处理5号的数据，但这次缺失下线数据的相关项目进行补全</p><p><img src="/2020/04/04/腾讯安全竞赛机器学习/8.png" alt="数据"></p><p>最后带回模型得到结果</p><p><img src="/2020/04/04/腾讯安全竞赛机器学习/9.png" alt="结果"></p><h1 id="折腾了一天，进不进得去决赛的呗，爱咋咋滴吧"><a href="#折腾了一天，进不进得去决赛的呗，爱咋咋滴吧" class="headerlink" title="折腾了一天，进不进得去决赛的呗，爱咋咋滴吧"></a>折腾了一天，进不进得去决赛的呗，爱咋咋滴吧</h1>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;腾讯安全竞赛机器学习&quot;&gt;&lt;a href=&quot;#腾讯安全竞赛机器学习&quot; class=&quot;headerlink&quot; title=&quot;腾讯安全竞赛机器学习&quot;&gt;&lt;/a&gt;腾讯安全竞赛机器学习&lt;/h2&gt;
    
    </summary>
    
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="腾讯安全" scheme="http://yoursite.com/tags/%E8%85%BE%E8%AE%AF%E5%AE%89%E5%85%A8/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>LSTM写诗</title>
    <link href="http://yoursite.com/2020/02/02/LSTM%E5%86%99%E8%AF%97/"/>
    <id>http://yoursite.com/2020/02/02/LSTM%E5%86%99%E8%AF%97/</id>
    <published>2020-02-02T07:01:30.000Z</published>
    <updated>2020-02-02T08:53:29.861Z</updated>
    
    <content type="html"><![CDATA[<h2 id="利用lstm制作的写诗模型"><a href="#利用lstm制作的写诗模型" class="headerlink" title="利用lstm制作的写诗模型"></a>利用lstm制作的写诗模型</h2><a id="more"></a><h2 id="拜某只蝙蝠所赐，这几天在家通过多方面学习排错写好了LSTM作诗模型"><a href="#拜某只蝙蝠所赐，这几天在家通过多方面学习排错写好了LSTM作诗模型" class="headerlink" title="拜某只蝙蝠所赐，这几天在家通过多方面学习排错写好了LSTM作诗模型"></a>拜某只蝙蝠所赐，这几天在家通过多方面学习排错写好了LSTM作诗模型</h2><h1 id="github网址：https-github-com-AAAAAimer-LSTM-poems"><a href="#github网址：https-github-com-AAAAAimer-LSTM-poems" class="headerlink" title="github网址：https://github.com/AAAAAimer/LSTM-poems"></a>github网址：<a href="https://github.com/AAAAAimer/LSTM-poems" target="_blank" rel="noopener">https://github.com/AAAAAimer/LSTM-poems</a></h1><h1 id="1-爬取某网站的七言诗句共10891条作为训练数据"><a href="#1-爬取某网站的七言诗句共10891条作为训练数据" class="headerlink" title="1.爬取某网站的七言诗句共10891条作为训练数据"></a>1.爬取某网站的七言诗句共10891条作为训练数据</h1><p><img src="/2020/02/02/LSTM写诗/诗词网站.png" alt="诗词网站"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from lxml import etree</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">urls = []</span><br><span class="line">headers = &#123;</span><br><span class="line">&quot;User-Agent&quot;:&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:72.0) Gecko/20100101 Firefox/72.0&quot;</span><br><span class="line">&#125;</span><br><span class="line">for id in range(100):</span><br><span class="line">print(id)</span><br><span class="line">for i in range(1,50):</span><br><span class="line">try:</span><br><span class="line">if i==1:</span><br><span class="line">new_url = &quot;http://www.shicimingju.com/chaxun/zuozhe/&quot; + str(id) + &quot;.html&quot;</span><br><span class="line">else:</span><br><span class="line">new_url = &quot;http://www.shicimingju.com/chaxun/zuozhe/&quot; + str(id) + &quot;_&quot; + str(i) + &quot;.html&quot;</span><br><span class="line">r = requests.get(new_url,headers=headers)</span><br><span class="line">for j in range(2,40,2):</span><br><span class="line">html = etree.HTML(r.text)</span><br><span class="line">index = html.xpath(&quot;/html/body/div[4]/div[1]/div[1]/div[&quot; + str(j) + &quot;]/div[2]/h3/a/@href&quot;)</span><br><span class="line">urls.append(index)</span><br><span class="line">except:</span><br><span class="line">pass</span><br><span class="line"></span><br><span class="line">poems = &quot;&quot;</span><br><span class="line">for i in urls:</span><br><span class="line">try:</span><br><span class="line">url = &quot;http://www.shicimingju.com&quot; + i[0]</span><br><span class="line">r = requests.get(url,headers=headers)</span><br><span class="line">soup = BeautifulSoup(r.text)</span><br><span class="line">poem = soup.find(&quot;div&quot;,attrs=&#123;&quot;class&quot;:&quot;item_content&quot;&#125;)</span><br><span class="line">for j in poem.contents:</span><br><span class="line">if len(j) == 16 and str(j)[7] == &apos;，&apos; and str(j)[0] != &apos;\n&apos; and str(j)[1] != &apos;\n&apos;:</span><br><span class="line">poems = poems + str(j) + &quot;\n&quot;</span><br><span class="line">print(str(j))</span><br><span class="line">except:</span><br><span class="line">pass</span><br><span class="line"></span><br><span class="line">with open(r&quot;data\poems_res.txt&quot;,&quot;a+&quot;,encoding=&quot;UTF-8&quot;) as f:</span><br><span class="line">f.write(poems)</span><br></pre></td></tr></table></figure><p><img src="/2020/02/02/LSTM写诗/1.png" alt="爬下来的诗词"><br>后面将以此文本作为训练集进行训练</p><h1 id="2-文本读取以及数据预处理"><a href="#2-文本读取以及数据预处理" class="headerlink" title="2.文本读取以及数据预处理"></a>2.文本读取以及数据预处理</h1><p>以每行为单位，给每行诗词得开头与结尾加上标记字符’B’和’E’</p><p>众所周知，在万物皆可用数字的形式表示，在图片处理中，我们将图片以矩阵的方式传入神经网络，而在文本处理中，我们要做的是将训练集中所有的字符赋予一个编码值<br>比如：“床前明月光，”，‘床’是0，‘前’是1…‘，’是5，而回车符也要占有一位编码，我们将文字数字化以后就可以将它传入神经网络进行训练了。</p><p>(1)定义读取并且编码的load_txt函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">start_mark = &apos;B&apos;</span><br><span class="line">end_mark = &apos;E&apos;</span><br><span class="line"></span><br><span class="line">def load_txt(file_name):</span><br><span class="line">res = &#123;&#125;</span><br><span class="line">txt = []</span><br><span class="line">with open(file_name, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:</span><br><span class="line">for line in f.readlines():</span><br><span class="line">try:</span><br><span class="line">content = start_mark + line + end_mark</span><br><span class="line">txt.append(content)</span><br><span class="line">except:</span><br><span class="line">pass</span><br><span class="line"></span><br><span class="line">all_vocab = [word for line in txt for word in line]</span><br><span class="line">vocabs = list(set(all_vocab))</span><br><span class="line">vocabs.append(&quot; &quot;)</span><br><span class="line">vocabs = list(set(vocabs))</span><br><span class="line">vocabs = sorted(vocabs)</span><br><span class="line"></span><br><span class="line">count = len(vocabs)</span><br><span class="line">int_to_word = dict(zip(range(count), vocabs))</span><br><span class="line">word_to_int = dict(zip(vocabs, range(count)))</span><br><span class="line"></span><br><span class="line">encoded = [list(map(lambda word: word_to_int.get(word, count), char)) for char in txt]</span><br><span class="line"></span><br><span class="line">res[&quot;encoded&quot;] = encoded</span><br><span class="line">res[&quot;all_vocab&quot;] = all_vocab</span><br><span class="line">res[&quot;vocabs&quot;] = vocabs</span><br><span class="line">res[&quot;count&quot;] = count</span><br><span class="line">res[&quot;int_to_word&quot;] = int_to_word</span><br><span class="line">res[&quot;word_to_int&quot;] = word_to_int</span><br><span class="line">return res</span><br></pre></td></tr></table></figure><p>结果以字典的形式返回，其中包括<br>encoded：编码后的文本内容<br>all_vocab：所有字符的列表(有重复)<br>vocabs：所有字符的列表(无重复)<br>count：字符总数<br>int_to_word：编码转字符字典<br>word_to_int：字符转编码字典</p><h3 id="划重点，影响了我好多时间的问题，在生成所有字符列表时切忌使用集合，因为集合是无序的，在生成文本时再读取文本编码会和之前不一样，保险起见set之后转回列表并排序"><a href="#划重点，影响了我好多时间的问题，在生成所有字符列表时切忌使用集合，因为集合是无序的，在生成文本时再读取文本编码会和之前不一样，保险起见set之后转回列表并排序" class="headerlink" title="划重点，影响了我好多时间的问题，在生成所有字符列表时切忌使用集合，因为集合是无序的，在生成文本时再读取文本编码会和之前不一样，保险起见set之后转回列表并排序"></a>划重点，影响了我好多时间的问题，在生成所有字符列表时切忌使用集合，因为集合是无序的，在生成文本时再读取文本编码会和之前不一样，保险起见set之后转回列表并排序</h3><p>(2)定义获得训练batch的函数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def get_batches(batch_size, word_vec):</span><br><span class="line">x_batches = []</span><br><span class="line">y_batches = []</span><br><span class="line">n = len(word_vec)//batch_size</span><br><span class="line">for i in range(n):</span><br><span class="line">b_index = i * batch_size</span><br><span class="line">e_index = b_index + batch_size</span><br><span class="line"></span><br><span class="line">batch = word_vec[b_index:e_index]</span><br><span class="line">max_len = max(map(len,word_vec))</span><br><span class="line">x_data = np.full((batch_size, max_len), 1, np.int32)</span><br><span class="line">for j in range(batch_size):</span><br><span class="line">for k in range(len(batch[j])):</span><br><span class="line">x_data[j][k] = batch[j][k]</span><br><span class="line">y_data = np.copy(x_data)</span><br><span class="line">y_data[:, :-1] = x_data[:, 1:]</span><br><span class="line"></span><br><span class="line">x_batches.append(x_data)</span><br><span class="line">y_batches.append(y_data)</span><br><span class="line">return x_batches, y_batches</span><br></pre></td></tr></table></figure></p><h1 id="3-模型搭建"><a href="#3-模型搭建" class="headerlink" title="3.模型搭建"></a>3.模型搭建</h1><p>模型主要分为<br>输入层-&gt;LSTM层-&gt;全连接层-&gt;输出层<br>将模型封装为类<br>首先在构造函数时将神经网络的所有参数传入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class lstm_model:</span><br><span class="line"></span><br><span class="line">def __init__(self, batch_size, lstm_size, num_layers, learning_rate, num_classes):</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">初始化对象是传入神经网络参数</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">self.batch_size = batch_size</span><br><span class="line">self.lstm_size = lstm_size</span><br><span class="line">self.num_layers = num_layers</span><br><span class="line">self.learning_rate = learning_rate</span><br><span class="line">self.num_classes = num_classes</span><br></pre></td></tr></table></figure></p><p>定义gen_model函数生成网络结构<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">def gen_model(self, training=True):</span><br><span class="line"></span><br><span class="line">    self.x_input = tf.placeholder(tf.int32, [self.batch_size, None])</span><br><span class="line">    #如果不是训练，标签输入层设为None</span><br><span class="line">    if training:</span><br><span class="line">        self.y_input = tf.placeholder(tf.int32, [self.batch_size, None])</span><br><span class="line">    else:</span><br><span class="line">        self.y_input = None</span><br><span class="line"></span><br><span class="line">    #定义LSTM层</span><br><span class="line">    cell = tf.contrib.rnn.BasicLSTMCell(self.lstm_size, state_is_tuple=True)</span><br><span class="line">    lstm = tf.contrib.rnn.MultiRNNCell([cell] * self.num_layers, state_is_tuple=True)</span><br><span class="line"></span><br><span class="line">    if training:</span><br><span class="line">        self.initial_state = lstm.zero_state(batch_size=self.batch_size,dtype=tf.float32)</span><br><span class="line">    else:</span><br><span class="line">        self.initial_state = lstm.zero_state(batch_size=self.batch_size, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    embedding = tf.get_variable(&apos;embedding&apos;, initializer=tf.random_uniform([self.num_classes + 1, self.lstm_size], -1.0, 1.0))</span><br><span class="line">    inputs = tf.nn.embedding_lookup(embedding, self.x_input)</span><br><span class="line"></span><br><span class="line">    lstm_outputs, self.final_state = tf.nn.dynamic_rnn(lstm, inputs, initial_state=self.initial_state)</span><br><span class="line">    lstm_outputs = tf.reshape(lstm_outputs, [-1, self.lstm_size])</span><br><span class="line"></span><br><span class="line">    full_weight = tf.Variable(tf.truncated_normal([self.lstm_size, self.num_classes + 1]))</span><br><span class="line">    full_bias = tf.Variable(tf.zeros(shape=[self.num_classes + 1]))</span><br><span class="line">    </span><br><span class="line">    self.logits = tf.nn.bias_add(tf.matmul(lstm_outputs, full_weight), bias=full_bias)</span><br><span class="line"></span><br><span class="line">    if training:</span><br><span class="line">        labels = tf.one_hot(tf.reshape(self.y_input, [-1]), depth=self.num_classes + 1)</span><br><span class="line"></span><br><span class="line">        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=self.logits))</span><br><span class="line"></span><br><span class="line">        self.train_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)</span><br><span class="line"></span><br><span class="line">    else:</span><br><span class="line">        self.pred = tf.nn.softmax(self.logits)</span><br></pre></td></tr></table></figure></p><p>对于较小的文本数据可以对输入直接选择进行独热编码，但对于字符较多的数据会增大神经网络运算量， 此处选择embedding对输入数据编码<br><img src="/2020/02/02/LSTM写诗/2.png" alt="embedding"></p><p>具体的网络结构不做过多解释，就是在原本循环神经网络的基础上加入一条state支线防止梯度爆炸和梯度消失的问题，反正LSTM层的具体结构咱也不明白</p><h1 id="4-训练"><a href="#4-训练" class="headerlink" title="4.训练"></a>4.训练</h1><p>废话不多说，开始训练<br><img src="/2020/02/02/LSTM写诗/3.png" alt="参数设定"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">txt_res = load_txt(txt_path)</span><br><span class="line">x_batches, y_batches = get_batches(batch_size, txt_res[&quot;encoded&quot;])</span><br><span class="line"></span><br><span class="line">model = lstm_model(batch_size=batch_size, lstm_size=lstm_size, num_layers=num_layers,</span><br><span class="line">learning_rate=learning_rate, num_classes=len(txt_res[&quot;vocabs&quot;]))</span><br><span class="line">model.gen_model(training=True)</span><br><span class="line"></span><br><span class="line">saver = tf.train.Saver(tf.global_variables())</span><br><span class="line">all_var = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">sess.run(all_var)</span><br><span class="line"></span><br><span class="line">print(&quot;开始训练&quot;)</span><br><span class="line"></span><br><span class="line">n = len(txt_res[&quot;encoded&quot;])//batch_size</span><br><span class="line">c = 0</span><br><span class="line">for e in range(epochs):</span><br><span class="line">for b in range(n):</span><br><span class="line">feed = &#123;model.x_input:x_batches[b],model.y_input:y_batches[b]&#125;</span><br><span class="line">loss, _, _ = sess.run([model.loss, model.final_state, model.train_optimizer],</span><br><span class="line">feed_dict=feed)</span><br><span class="line">print(&quot;epoch:%d batch:%d counter:%d loss:%.8f&quot; % (e,b,c,loss))</span><br><span class="line">c += 1</span><br><span class="line">if e % 9 == 0:</span><br><span class="line">saver.save(sess, os.path.join(checkpoint, model_prefix), global_step=e)</span><br></pre></td></tr></table></figure><h1 id="5-生成文本"><a href="#5-生成文本" class="headerlink" title="5.生成文本"></a>5.生成文本</h1><p>训练结束后就可以开始作诗了，原理很简单，每次传入一个字符和上次训练留下的state，把每个输出结果利用numpy的概率函数取出字符的索引当做编码找到相应的字符就可以了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">def get_word(pred, vocabs):</span><br><span class="line">pred = pred[0]       </span><br><span class="line">pred /= np.sum(pred)</span><br><span class="line">index = np.random.choice(np.arange(len(pred)), p=pred)</span><br><span class="line">return vocabs[index]</span><br><span class="line"></span><br><span class="line">start_mark = &apos;B&apos;</span><br><span class="line">end_mark = &apos;E&apos;</span><br><span class="line">checkpoint = &quot;checkpoint/&quot;</span><br><span class="line">txt_path = &quot;data/poems_res.txt&quot;</span><br><span class="line">batch_size = 1</span><br><span class="line">learning_rate = 0.01</span><br><span class="line">lstm_size = 128</span><br><span class="line">num_layers = 2</span><br><span class="line"></span><br><span class="line">txt_res = load_txt(txt_path)</span><br><span class="line"></span><br><span class="line">model = lstm_model(batch_size=batch_size, lstm_size=lstm_size,</span><br><span class="line">num_layers=num_layers, learning_rate=learning_rate, num_classes=len(txt_res[&quot;vocabs&quot;]))</span><br><span class="line">model.gen_model(training=False)</span><br><span class="line"></span><br><span class="line">saver = tf.train.Saver(tf.global_variables())</span><br><span class="line">all_var = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())</span><br><span class="line"></span><br><span class="line">first_word = input(&quot;输入第一个字：&quot;)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">sess.run(all_var)</span><br><span class="line">checkpoint = tf.train.latest_checkpoint(checkpoint)</span><br><span class="line">saver.restore(sess, checkpoint)</span><br><span class="line"></span><br><span class="line">x = np.array([[txt_res[&quot;word_to_int&quot;][start_mark]]])</span><br><span class="line"></span><br><span class="line">feed = &#123;model.x_input:x&#125;</span><br><span class="line">pred, last_state = sess.run([model.pred,model.final_state], feed_dict=feed)</span><br><span class="line">x = np.array([[txt_res[&quot;word_to_int&quot;][first_word]]])</span><br><span class="line">word = first_word</span><br><span class="line">txt = &quot;&quot;</span><br><span class="line"></span><br><span class="line">c = 1</span><br><span class="line">while word != end_mark:</span><br><span class="line">txt = txt + word</span><br><span class="line">if c &gt; 1000:</span><br><span class="line">break</span><br><span class="line">feed = &#123;model.x_input:x, model.initial_state:last_state&#125;</span><br><span class="line">pred, last_state = sess.run([model.pred,model.final_state], feed_dict=feed)</span><br><span class="line">word = get_word(pred, txt_res[&quot;vocabs&quot;])</span><br><span class="line">x = np.array([[txt_res[&quot;word_to_int&quot;][word]]])</span><br><span class="line">c += 1</span><br><span class="line"></span><br><span class="line">print(txt)</span><br></pre></td></tr></table></figure><p>随便选一个字，比如李白最喜欢的月字</p><p><img src="/2020/02/02/LSTM写诗/4.png" alt="结果展示"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;利用lstm制作的写诗模型&quot;&gt;&lt;a href=&quot;#利用lstm制作的写诗模型&quot; class=&quot;headerlink&quot; title=&quot;利用lstm制作的写诗模型&quot;&gt;&lt;/a&gt;利用lstm制作的写诗模型&lt;/h2&gt;
    
    </summary>
    
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="LSTM" scheme="http://yoursite.com/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>GAN</title>
    <link href="http://yoursite.com/2019/11/07/GAN(fashion%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%BA%E4%BE%8B)/"/>
    <id>http://yoursite.com/2019/11/07/GAN(fashion%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%BA%E4%BE%8B)/</id>
    <published>2019-11-07T09:59:18.000Z</published>
    <updated>2019-11-08T12:56:59.388Z</updated>
    
    <content type="html"><![CDATA[<h2 id="以fashion数据集为例的GAN网络"><a href="#以fashion数据集为例的GAN网络" class="headerlink" title="以fashion数据集为例的GAN网络"></a>以fashion数据集为例的GAN网络</h2><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">from PIL import Image</span><br><span class="line">import scipy</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line">fashion = input_data.read_data_sets(r&quot;D:\mnsit\fashion&quot;,one_hot=True)#读取fashion数据集</span><br><span class="line"></span><br><span class="line">#def get_image_batch():</span><br><span class="line">#imgs = []</span><br><span class="line">#for i in range(50):</span><br><span class="line">#ran = random.randint(0,90)</span><br><span class="line">#img = Image.open(images[ran])</span><br><span class="line">#img = img.convert(&quot;L&quot;) </span><br><span class="line">#img = img.resize([64,64])</span><br><span class="line">#data = img.getdata()</span><br><span class="line">#data = np.array(data)</span><br><span class="line">#data = data/255</span><br><span class="line">#data = np.reshape(data,(4096,))</span><br><span class="line">#imgs.append(data)</span><br><span class="line">#return np.array(imgs)</span><br><span class="line"></span><br><span class="line">def xavier_init(size):#利用xavier初始化参数</span><br><span class="line">in_dim = size[0]</span><br><span class="line">xavier_stddev = 1. / tf.sqrt(in_dim / 2.)</span><br><span class="line">return tf.random_normal(shape=size, stddev=xavier_stddev)</span><br><span class="line"></span><br><span class="line">Z = tf.placeholder(tf.float32,shape=[None,100],name=&apos;Z&apos;)</span><br><span class="line"></span><br><span class="line">G_W1 = tf.Variable(xavier_init([100,256]),name=&apos;G_W1&apos;)</span><br><span class="line">G_b1 = tf.Variable(tf.zeros(shape=[256]),name=&apos;G_b1&apos;)</span><br><span class="line">G_W2 = tf.Variable(xavier_init([256,784]),name=&apos;G_W2&apos;)</span><br><span class="line">G_b2 = tf.Variable(tf.zeros(shape=[784]),name=&apos;G_b2&apos;)</span><br><span class="line"># G_W3 = tf.Variable(xavier_init([512,1024]),name=&apos;G_W3&apos;)</span><br><span class="line"># G_b3 = tf.Variable(tf.zeros(shape=[1024]),name=&apos;G_b3&apos;)</span><br><span class="line"># G_W4 = tf.Variable(xavier_init([1024,2048]),name=&apos;G_W4&apos;)</span><br><span class="line"># G_b4 = tf.Variable(tf.zeros(shape=[2048]),name=&apos;G_b4&apos;)</span><br><span class="line"># G_W5 = tf.Variable(xavier_init([2048,4096]),name=&apos;G_W5&apos;)</span><br><span class="line"># G_b5 = tf.Variable(tf.zeros(shape=[4096]),name=&apos;G_b5&apos;)</span><br><span class="line">theta_G = [G_W1,G_b1,G_W2,G_b2]</span><br><span class="line"></span><br><span class="line">def generator(z):</span><br><span class="line">G_h1 = tf.nn.relu(tf.matmul(z,G_W1) + G_b1)</span><br><span class="line">#     G_h2 = tf.nn.relu(tf.matmul(G_h1,G_W2) + G_b2)</span><br><span class="line">#     G_h3 = tf.nn.relu(tf.matmul(G_h2,G_W3) + G_b3)</span><br><span class="line">#     G_h4 = tf.nn.relu(tf.matmul(G_h3,G_W4) + G_b4)</span><br><span class="line">G_log_prob = tf.matmul(G_h1,G_W2) + G_b2</span><br><span class="line">G_prob = tf.nn.sigmoid(G_log_prob)</span><br><span class="line">   </span><br><span class="line">   return G_prob</span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32,shape=[None,784],name=&apos;X&apos;)</span><br><span class="line"></span><br><span class="line">D_W1 = tf.Variable(xavier_init([784,256]),name=&apos;D_W1&apos;)</span><br><span class="line">D_b1 = tf.Variable(tf.zeros(shape=[256]),name=&apos;D_b1&apos;)</span><br><span class="line">D_W2 = tf.Variable(xavier_init([256,1]),name=&apos;D_W2&apos;)</span><br><span class="line">D_b2 = tf.Variable(tf.zeros(shape=[1]),name=&apos;D_b2&apos;)</span><br><span class="line"># D_W3 = tf.Variable(xavier_init([1024,512]),name=&apos;D_W3&apos;)</span><br><span class="line"># D_b3 = tf.Variable(tf.zeros(shape=[512]),name=&apos;D_b3&apos;)</span><br><span class="line"># D_W4 = tf.Variable(xavier_init([512,256]),name=&apos;D_W4&apos;)</span><br><span class="line"># D_b4 = tf.Variable(tf.zeros(shape=[256]),name=&apos;D_b4&apos;)</span><br><span class="line"># D_W5 = tf.Variable(xavier_init([256,128]),name=&apos;D_W5&apos;)</span><br><span class="line"># D_b5 = tf.Variable(tf.zeros(shape=[128]),name=&apos;D_b5&apos;)</span><br><span class="line"># D_W6 = tf.Variable(xavier_init([128,1]),name=&apos;D_W6&apos;)</span><br><span class="line"># D_b6 = tf.Variable(tf.zeros(shape=[1]),name=&apos;D_b6&apos;)</span><br><span class="line">theta_D = [D_W1,D_b1,D_W2,D_b2]</span><br><span class="line"></span><br><span class="line">def descriminator(x):</span><br><span class="line">D_h1 = tf.nn.relu(tf.matmul(x,D_W1) + D_b1)</span><br><span class="line">#     D_h2 = tf.nn.relu(tf.matmul(D_h1,D_W2) + D_b2)</span><br><span class="line">#     D_h3 = tf.nn.relu(tf.matmul(D_h2,D_W3) + D_b3)</span><br><span class="line">#     D_h4 = tf.nn.relu(tf.matmul(D_h3,D_W4) + D_b4)</span><br><span class="line">#     D_h5 = tf.nn.relu(tf.matmul(D_h4,D_W5) + D_b5)</span><br><span class="line">D_log_prob = tf.matmul(D_h1, D_W2) + D_b2</span><br><span class="line">D_prob = tf.nn.sigmoid(D_log_prob)</span><br><span class="line">   </span><br><span class="line">   return D_prob, D_log_prob</span><br><span class="line"></span><br><span class="line">G_sample = generator(Z)</span><br><span class="line"></span><br><span class="line">D_real, D_log_real = descriminator(X)</span><br><span class="line">D_fake, D_log_fake = descriminator(G_sample)</span><br><span class="line"></span><br><span class="line">D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1. - D_fake))</span><br><span class="line">G_loss = -tf.reduce_mean(tf.log(D_fake))</span><br><span class="line"></span><br><span class="line">D_solver = tf.train.AdadeltaOptimizer(learning_rate=0.1).minimize(D_loss, var_list=theta_D)</span><br><span class="line">G_solver = tf.train.AdadeltaOptimizer(learning_rate=0.1).minimize(G_loss, var_list=theta_G)</span><br><span class="line"></span><br><span class="line">def sample_Z(m,n):</span><br><span class="line">return np.random.uniform(-1.,1.,size=[m,n])</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">def save_images(data,size,path):</span><br><span class="line">data = np.resize(data,[size[0],size[1]])</span><br><span class="line">data = data * 255</span><br><span class="line">new_im = Image.fromarray(data)</span><br><span class="line">new_im = new_im.convert(&apos;L&apos;)</span><br><span class="line">new_im.save(path)</span><br><span class="line"></span><br><span class="line">for i in range(60000):</span><br><span class="line">X_mb,_ = fashion.train.next_batch(50)</span><br><span class="line"></span><br><span class="line">_, D_loss_curr = sess.run([D_solver,D_loss], feed_dict=&#123;X:X_mb, Z:sample_Z(50,100)&#125;)</span><br><span class="line">_, G_loss_curr = sess.run([G_solver,G_loss], feed_dict=&#123;Z:sample_Z(50,100)&#125;)</span><br><span class="line">print(i)</span><br><span class="line">if i%1000==0 :</span><br><span class="line">print(&quot;经过第&quot; + str(i) + &quot;次:&quot;)</span><br><span class="line">print(&quot;识别器损失函数：&quot;)</span><br><span class="line">print(sess.run(D_loss, feed_dict=&#123;X:X_mb, Z:sample_Z(50,100)&#125;))</span><br><span class="line">print(&quot;生成器损失函数：&quot;)</span><br><span class="line">print(sess.run(G_loss, feed_dict=&#123;Z:sample_Z(50,100)&#125;))</span><br><span class="line"></span><br><span class="line">print()</span><br><span class="line">if i%1000 == 0:</span><br><span class="line">G_image = sess.run(G_sample,feed_dict=&#123;X:X_mb, Z:sample_Z(50,100)&#125;)</span><br><span class="line">print(i)</span><br><span class="line">print(G_image)</span><br><span class="line">save_images(G_image,[196,196],&quot;D://pic//&quot; + str(i) + &quot;.jpg&quot;)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;以fashion数据集为例的GAN网络&quot;&gt;&lt;a href=&quot;#以fashion数据集为例的GAN网络&quot; class=&quot;headerlink&quot; title=&quot;以fashion数据集为例的GAN网络&quot;&gt;&lt;/a&gt;以fashion数据集为例的GAN网络&lt;/h2&gt;
    
    </summary>
    
    
    
      <category term="GAN" scheme="http://yoursite.com/tags/GAN/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>JAVA HashMap的排序</title>
    <link href="http://yoursite.com/2019/10/17/JAVA-HashMap%E7%9A%84%E6%8E%92%E5%BA%8F/"/>
    <id>http://yoursite.com/2019/10/17/JAVA-HashMap%E7%9A%84%E6%8E%92%E5%BA%8F/</id>
    <published>2019-10-17T10:26:28.000Z</published>
    <updated>2019-10-17T10:46:47.923Z</updated>
    
    <content type="html"><![CDATA[<h2 id="JAVA-HashMap的排序"><a href="#JAVA-HashMap的排序" class="headerlink" title="JAVA HashMap的排序"></a>JAVA HashMap的排序</h2><a id="more"></a><p>众所周知，字典是没有顺序的，所以我们在Java中应该如何给字典按值排序呢？</p><h1 id="以今天做的一道题为实例"><a href="#以今天做的一道题为实例" class="headerlink" title="以今天做的一道题为实例"></a>以今天做的一道题为实例</h1><p><img src="/2019/10/17/JAVA-HashMap的排序/1.png" alt="题目"><br><img src="/2019/10/17/JAVA-HashMap的排序/2.png" alt="题目"><br>首先将每个人与他对应的收入存到一个HashMap中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Map&lt;Integer, Float&gt; person = new HashMap&lt;Integer,Float&gt;();  //定义一个HashMap</span><br><span class="line"></span><br><span class="line">for(int i=0;i&lt;n;i++) &#123;</span><br><span class="line">person.put(i, (float) 0.0);//初始化</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">float sum = 0;//统计每个人发的钱</span><br><span class="line"></span><br><span class="line">for(int i=0;i&lt;n;i++) &#123;</span><br><span class="line">int m = in.nextInt();</span><br><span class="line"></span><br><span class="line">for(int j=0;j&lt;m;j++) &#123;</span><br><span class="line">int id = in.nextInt();//收钱人的编号</span><br><span class="line">float money = in.nextFloat()/100;//收钱数量</span><br><span class="line">float tmp = person.get(id-1);</span><br><span class="line">person.put(id-1,tmp+money);</span><br><span class="line">sum += money;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">float tmp = person.get(i);</span><br><span class="line">person.put(i, tmp-sum);</span><br><span class="line">sum = 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>下面引入Map.Entry的概念：<br>Map的entrySet()方法返回一个实现Map.Entry接口的对象集合。集合中每个对象都是底层Map中一个特定的键/值对。通过这个集合的迭代器，获得每一个条目(唯一获取方式)的键或值并对值进行更改。<br>我们只需要利用Map的entrySet方法就可以把HashMap类型的容器转换为Map.Entry的容器，随后把它放到一个列表里就可以排序了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Map.Entry&lt;Integer,Float&gt;&gt; mlist = new ArrayList&lt;&gt;();//定义装Map.Entry的容器</span><br><span class="line"></span><br><span class="line">for(Map.Entry&lt;Integer, Float&gt; m : person.entrySet()) &#123;</span><br><span class="line">mlist.add(m);//将Map.Entry放入容器</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">mlist.sort(new Comparator&lt;Map.Entry&lt;Integer, Float&gt;&gt;() &#123;//排序并重写排序方法</span><br><span class="line">@Override</span><br><span class="line">public int compare(Entry&lt;Integer, Float&gt; o1, Entry&lt;Integer, Float&gt; o2) &#123;</span><br><span class="line">// TODO Auto-generated method stub</span><br><span class="line">if(o1.getValue()==o2.getValue()) &#123;//在数值相同时，选择比较编号的大小</span><br><span class="line">return (o1.getKey().compareTo(o2.getKey()));</span><br><span class="line">&#125;else &#123;</span><br><span class="line">return -(o1.getValue().compareTo(o2.getValue()));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></p><p>最后输出排序后的结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for(int i=0;i&lt;n;i++) &#123;</span><br><span class="line">System.out.printf(&quot;%d %.2f&quot;,mlist.get(i).getKey()+1,mlist.get(i).getValue());</span><br><span class="line">System.out.println();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1 11.63</span><br><span class="line">2 3.63</span><br><span class="line">8 3.63</span><br><span class="line">3 2.11</span><br><span class="line">7 1.69</span><br><span class="line">6 -1.67</span><br><span class="line">9 -2.18</span><br><span class="line">10 -3.26</span><br><span class="line">5 -3.26</span><br><span class="line">4 -12.32</span><br></pre></td></tr></table></figure><p>完整代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">import java.math.RoundingMode;</span><br><span class="line">import java.text.NumberFormat;</span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.Comparator;</span><br><span class="line">import java.util.HashMap;</span><br><span class="line">import java.util.List;</span><br><span class="line">import java.util.Map;</span><br><span class="line">import java.util.Map.Entry;</span><br><span class="line">import java.util.Scanner;</span><br><span class="line"></span><br><span class="line">public class Main &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">Scanner in = new Scanner(System.in);</span><br><span class="line"></span><br><span class="line">int n = in.nextInt();</span><br><span class="line"></span><br><span class="line">Map&lt;Integer, Float&gt; person = new HashMap&lt;Integer,Float&gt;();</span><br><span class="line"></span><br><span class="line">for(int i=0;i&lt;n;i++) &#123;</span><br><span class="line">person.put(i, (float) 0.0);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">float sum = 0;</span><br><span class="line"></span><br><span class="line">for(int i=0;i&lt;n;i++) &#123;</span><br><span class="line">int m = in.nextInt();</span><br><span class="line"></span><br><span class="line">for(int j=0;j&lt;m;j++) &#123;</span><br><span class="line">int id = in.nextInt();</span><br><span class="line">float money = in.nextFloat()/100;</span><br><span class="line">float tmp = person.get(id-1);</span><br><span class="line">person.put(id-1,tmp+money);</span><br><span class="line">sum += money;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">float tmp = person.get(i);</span><br><span class="line">person.put(i, tmp-sum);</span><br><span class="line">sum = 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">List&lt;Map.Entry&lt;Integer,Float&gt;&gt; mlist = new ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">for(Map.Entry&lt;Integer, Float&gt; m : person.entrySet()) &#123;</span><br><span class="line">mlist.add(m);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">mlist.sort(new Comparator&lt;Map.Entry&lt;Integer, Float&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public int compare(Entry&lt;Integer, Float&gt; o1, Entry&lt;Integer, Float&gt; o2) &#123;</span><br><span class="line">// TODO Auto-generated method stub</span><br><span class="line">if(o1.getValue()==o2.getValue()) &#123;</span><br><span class="line">return (o1.getKey().compareTo(o2.getKey()));</span><br><span class="line">&#125;else &#123;</span><br><span class="line">return -(o1.getValue().compareTo(o2.getValue()));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">for(int i=0;i&lt;n;i++) &#123;</span><br><span class="line">System.out.printf(&quot;%d %.2f&quot;,mlist.get(i).getKey()+1,mlist.get(i).getValue());</span><br><span class="line">System.out.println();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;JAVA-HashMap的排序&quot;&gt;&lt;a href=&quot;#JAVA-HashMap的排序&quot; class=&quot;headerlink&quot; title=&quot;JAVA HashMap的排序&quot;&gt;&lt;/a&gt;JAVA HashMap的排序&lt;/h2&gt;
    
    </summary>
    
    
    
      <category term="JAVA" scheme="http://yoursite.com/tags/JAVA/"/>
    
      <category term="Hashmap" scheme="http://yoursite.com/tags/Hashmap/"/>
    
  </entry>
  
  <entry>
    <title>Python给爷爬京东评论</title>
    <link href="http://yoursite.com/2019/09/04/Python%E7%BB%99%E7%88%B7%E7%88%AC%E4%BA%AC%E4%B8%9C%E8%AF%84%E8%AE%BA/"/>
    <id>http://yoursite.com/2019/09/04/Python%E7%BB%99%E7%88%B7%E7%88%AC%E4%BA%AC%E4%B8%9C%E8%AF%84%E8%AE%BA/</id>
    <published>2019-09-04T08:07:50.000Z</published>
    <updated>2019-09-04T08:51:15.589Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Python爬取京东评论"><a href="#Python爬取京东评论" class="headerlink" title="Python爬取京东评论"></a>Python爬取京东评论</h2><a id="more"></a><p>利用Python 爪巴耳又 京东商品评论</p><h1 id="爪巴-前分析"><a href="#爪巴-前分析" class="headerlink" title="爪巴 前分析"></a>爪巴 前分析</h1><p>爷把爪牙伸向了买不起的Beats耳机，首先看一下评论区的源码<br><img src="/2019/09/04/Python给爷爬京东评论/0.png" alt="评论区的源码"><br>这里是一个动态获取的json字符串，如果直接用url爬的话会爬下来一堆加载中，所以需要进行动态的分析</p><p>谷歌自带的抓包功能可以很好的找到请求包，右键检查，然后选择network，直接搜索‘comment’找一找就有了<br><img src="/2019/09/04/Python给爷爬京东评论/1.png" alt="谷歌"><br>然后找到他的请求链接<br><img src="/2019/09/04/Python给爷爬京东评论/2.png" alt="url"><br>我们可以看到，这里request下来的是一个json字符串，这样好说了，连BeautifulSoup解析都用不找了<br>一开始我直接get的这个url爬下来的是这样的<br><img src="/2019/09/04/Python给爷爬京东评论/3.jpg" alt="直接 爪巴"><br>这也是京东的反爬虫措施吧，一定是请求头少了什么东西，后来我把cookie加上也没有用，最后直接把请求头里的所有东西都扔了进去。<br><img src="/2019/09/04/Python给爷爬京东评论/4.png" alt="请求头"><br><img src="/2019/09/04/Python给爷爬京东评论/5.png" alt="请求头"><br>最后爬下来的json字符串<br>里面评论的key值是’comment’<br><img src="/2019/09/04/Python给爷爬京东评论/6.png" alt="结果"><br>如图，开头还有别的字符(‘fetchJSON_comment98vv2115(‘)，用lstrip和rstrip把它去掉就可以了<br>另外url中page表示的评论的页数，于是用俩个循环嵌套就可以爬取多页的内容了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">for i in range(0,10):     #爬取10页的内容</span><br><span class="line">url = &apos;https://sclub.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv2115&amp;productId=7824797&amp;score=0&amp;sortType=5&amp;page=&apos; + str(i) + &apos;&amp;pageSize=10&amp;isShadowSku=0&amp;fold=1&apos;      #i是评论的页数</span><br><span class="line">r = requests.get(url, headers=headers)    #请求并传入headers的值</span><br><span class="line">text = json.loads(r.text.lstrip(&apos;fetchJSON_comment98vv2115(&apos;).rstrip(&apos;);&apos;))</span><br><span class="line">for i in text[&apos;comments&apos;]:</span><br><span class="line">print(b,&quot;:&quot;,i[&apos;content&apos;])</span><br><span class="line">b = b + 1      #定义一个b变量来做计数君</span><br><span class="line">time.sleep(2)     #每页爬取的时候中途停止2秒</span><br></pre></td></tr></table></figure></p><p>最终结果<br><img src="/2019/09/04/Python给爷爬京东评论/7.png" alt="结果"></p><h2 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">import json</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">&apos;authority&apos;: &apos;sclub.jd.com&apos;,</span><br><span class="line">&apos;method&apos;: &apos;GET&apos;,</span><br><span class="line">&apos;path&apos;: &apos;/comment/productPageComments.action?callback=fetchJSON_comment98vv2115&amp;productId=7824797&amp;score=0&amp;sortType=5&amp;page=0&amp;pageSize=10&amp;isShadowSku=0&amp;fold=1&apos;,</span><br><span class="line">&apos;scheme&apos;: &apos;https&apos;,</span><br><span class="line">&apos;accept&apos;: &apos;*/*&apos;,</span><br><span class="line">&apos;accept-encoding&apos;: &apos;gzip, deflate, br&apos;,</span><br><span class="line">&apos;accept-language&apos;: &apos;zh-CN,zh;q=0.8&apos;,</span><br><span class="line">&apos;cahce-control&apos;: &apos;no-cache&apos;,</span><br><span class="line">&apos;cookie&apos;: &apos;shshshfpa=724ff748-c09c-07a4-690b-c530bd4e234d-1530001150; areaId=13; ipLoc-djd=13-1112-46666-0; PCSYCityID=CN_370000_370900_370902; user-key=bcbe746b-bd12-459c-9cb8-045ad278ea79; unpl=V2_ZzNtbUtXEEYnDRVUKE1VVWILFl4RVERGcgBCVH0aVAJvBUIJclRCFX0UR1xnGFkUZwQZXUBcRxBFCEdkexhdBGcHGllBUnMldDhFVEsRbAFiBBZVQlNLF0U4QWRLGVkAbwEWX0ZSXxZwD0RceClaBmcAE1VBVUoldDhGV3gQWQRuAhJfchwtFDgMQ1N%2fEVwBbwEiXHJU; cn=0; _pst=jd_620cddcd73a7b; unick=jd_156620pmr; pin=jd_620cddcd73a7b; thor=30A9230ACAE426D9C78A61CA7837F8D4A77BFA4F3607E3425FD97285B18FFEDF7D372E2534A97BA24CE61F470A8CB4A7B6CE99A44EDEFBD8B8D4856643B149AB0FFB2E9577084D09FA8EA5545746BBDF53BBD14E299935AB79C8717385446A36E543845F9902FAF9E83E0A41A599115969A77C7556C9FFF82AFD004B093B81EED7196D38448FBA76BB93CD90ECA1DBB8A4DE49041B2D4B5313D1FEB6760B337B; _tp=B%2BRjxGpLYGiCYbbWQ3shnXaN1fmT5iioD2%2BLJab0QNI%3D; pinId=8MwzGUoDS8UZAU3ai6-NlLV9-x-f3wj7; __jdv=76161171|buy.jiegeng.com|t_1000159524_|tuiguang|81bbc4b1be8a4952b26b6951729697ae|1567584853053; shshshfp=e3e6a9aab1f87ae8815309aa3e941e39; shshshsID=48dcc30254839ca93b4bfeaa57d55b4f_7_1567584939419; shshshfpb=17d0f1d7f357e402fbd82bc9ae4377ed7a0e7b4804022131b5b31f6f8e; __jda=122270672.546591593.1549889886.1567513578.1567584839.11; __jdb=122270672.15.546591593|11.1567584839; __jdc=122270672; 3AB9D23F7A4B3C9B=YEQZE4K5VITVEWIDGLZYFPVYLSZMBCW3G4PE3V7T7VJGQXWUSM4S4CUXEI4IUGGORLSAJ2DFODWXLNZH75FBMPP54Q; __jdu=546591593&apos;,</span><br><span class="line">&apos;pragma&apos;: &apos;no-cache&apos;,</span><br><span class="line">&apos;referer&apos;: &apos;https://item.jd.com/7824797.html&apos;,</span><br><span class="line">&apos;user-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36&apos;,</span><br><span class="line">&#125;</span><br><span class="line">b = 1     #计数君</span><br><span class="line"></span><br><span class="line">for i in range(0,10):</span><br><span class="line">url = &apos;https://sclub.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv2115&amp;productId=7824797&amp;score=0&amp;sortType=5&amp;page=&apos; + str(i) + &apos;&amp;pageSize=10&amp;isShadowSku=0&amp;fold=1&apos;</span><br><span class="line">r = requests.get(url, headers=headers)    #请求并传入headers的值</span><br><span class="line">text = json.loads(r.text.lstrip(&apos;fetchJSON_comment98vv2115(&apos;).rstrip(&apos;);&apos;))</span><br><span class="line">for i in text[&apos;comments&apos;]:</span><br><span class="line">print(b,&quot;:&quot;,i[&apos;content&apos;])</span><br><span class="line">b = b + 1      #生成b来做一个计数君</span><br><span class="line">time.sleep(2)     #没页爬取的时候中途停止2秒</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Python爬取京东评论&quot;&gt;&lt;a href=&quot;#Python爬取京东评论&quot; class=&quot;headerlink&quot; title=&quot;Python爬取京东评论&quot;&gt;&lt;/a&gt;Python爬取京东评论&lt;/h2&gt;
    
    </summary>
    
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>wxml调用js的函数</title>
    <link href="http://yoursite.com/2019/05/20/wxml%E8%B0%83%E7%94%A8js%E7%9A%84%E5%87%BD%E6%95%B0/"/>
    <id>http://yoursite.com/2019/05/20/wxml%E8%B0%83%E7%94%A8js%E7%9A%84%E5%87%BD%E6%95%B0/</id>
    <published>2019-05-20T03:18:03.000Z</published>
    <updated>2019-05-20T03:48:37.195Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;这几天一直在整微信小程序，一直很头疼的是如何在wxml调用js里的有参函数，因为button标签里的bindtap属性直接使用函数名，<br><a id="more"></a><br>所以没法传参，但是js提供了，函数中会默认传入标签的参数，所以可以利用它来做文章。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">addfood: function (res) &#123;</span><br><span class="line">console.log(res)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>&emsp;&emsp;直接打印一下res参数查看一下<br><img src="/2019/05/20/wxml调用js的函数/1.png" alt="打印res"><br>大体浏览了一下我注意到了在target下面有个dataset参数<br><img src="/2019/05/20/wxml调用js的函数/2.png" alt="dataset"><br>由此可以理解，可以通过标签自带的一些属性来实现wxml对于js的参数传递，只是想办法寻找用什么来传递就好了。<br>于是我xjbd看了看button标签的属性，发现了data-属性<br><img src="/2019/05/20/wxml调用js的函数/3.png" alt="data-"><br>给data-赋值后，再打印res看到了结果中有一个空的键值对应了我刚刚写的值<br><img src="/2019/05/20/wxml调用js的函数/5.png" alt="data的值"><br><img src="/2019/05/20/wxml调用js的函数/4.png" alt="data的值"><br>于是机智的我光速明白了data-后面的”-“后是键，后面=跟上他的value值就好了<br>随后按图赋值<br><img src="/2019/05/20/wxml调用js的函数/6.png" alt="data的值"><br>再打印就可以看到res的dataset值<br><img src="/2019/05/20/wxml调用js的函数/7.png" alt="data的值"><br>然后在函数里通过res.target.dataset.键就可以得到这些数据了</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;emsp;&amp;emsp;这几天一直在整微信小程序，一直很头疼的是如何在wxml调用js里的有参函数，因为button标签里的bindtap属性直接使用函数名，&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="wxml" scheme="http://yoursite.com/tags/wxml/"/>
    
      <category term="js" scheme="http://yoursite.com/tags/js/"/>
    
  </entry>
  
  <entry>
    <title>看着整整神经网络</title>
    <link href="http://yoursite.com/2019/05/05/%E7%9C%8B%E7%9D%80%E6%95%B4%E6%95%B4%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://yoursite.com/2019/05/05/%E7%9C%8B%E7%9D%80%E6%95%B4%E6%95%B4%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</id>
    <published>2019-05-05T12:30:37.000Z</published>
    <updated>2019-05-09T09:30:21.621Z</updated>
    
    <content type="html"><![CDATA[<h1 id="初识神经网络"><a href="#初识神经网络" class="headerlink" title="初识神经网络"></a>初识神经网络</h1><p>这篇文章主要对神经网络进行一下简单的概述，毕竟如果把里面的每一个点拉出来都够讲好久<br><a id="more"></a><br>本文主要参考来源：<a href="https://blog.csdn.net/young2415/article/details/81772068" target="_blank" rel="noopener">https://blog.csdn.net/young2415/article/details/81772068</a></p><p>文章中实现了一个简单的二维输入和输出的神经网络<br><img src="/2019/05/05/看着整整神经网络/1.png" alt="示意图"></p><p>一个神经网络主要由输入层，隐藏层与输出层组成</p><h2 id="如何去理解？"><a href="#如何去理解？" class="headerlink" title="如何去理解？"></a>如何去理解？</h2><p>1.比如说我们看到了一只小狗（输入层获得了动物的基本信息，如毛发颜色）</p><p>2.随后我们的神经元进行处理（隐藏层将输入的图像信息处理为数字信息）</p><p>3.最后，我们得到结果，它99.9%是只狗，0.1%是只猫（输出层输出结果）</p><p>文中生成了二维的数据，最终被归为了俩类（利用红色与蓝色标记）<br><img src="/2019/05/05/看着整整神经网络/2.png" alt="数据"><br>即隐藏层通过处理输入层输入的二维数据得到了另一组二维数据（即每一种类型的概率）最后从输入层得到结果<br>数据生成代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 生成数据集并绘制出来</span><br><span class="line">np.random.seed(0)</span><br><span class="line">X, y = sklearn.datasets.make_moons(200, noise=0.20)</span><br><span class="line">plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)</span><br></pre></td></tr></table></figure></p><h1 id="神经网络如何实现预测"><a href="#神经网络如何实现预测" class="headerlink" title="神经网络如何实现预测"></a>神经网络如何实现预测</h1><p>神经网络预测的关键就在于隐藏层对于输入数据的处理，我们的理想结果是我们传入一个类型1的数据，最后他的计算结果是[1,0]（当然这是不可能的），类型2的结果就是[0,1]，这需要我们不断地去调整隐藏层中公式的参数让正确的类型的概率尽可能的高。<br>通俗的讲，我们把传入的数据通过特定的函数将其生成为一个概率值，即预测结果，一个二维数值，俩个数相加等于1，这个特定的函数称为激活函数</p><script type="math/tex; mode=display">z_1 = xW_1+b_1</script><script type="math/tex; mode=display">a_1 = tanh(z_1)</script><script type="math/tex; mode=display">z_2 = a_1W_2+b_2 $​$$$ a_2 = y = softmax(z_2)</script><p>z表示每一层的输入,a表示激活函数后的输出，至于他的参数各在那一层我也很迷惑，后来看了它每一层矩阵的维度才大体理解，此处的W参数并不是一个单个的数，而是一个矩阵向量，输入的数据与W1点乘改变为隐藏层的维度。<br>例如文中的20000个二维数据与W1（2x3的矩阵）相乘以后生成了一个20000x3的矩阵，然后就是通过隐藏层的激活函数将数据规范化，随后通过W2再将数据转换为与输出维度相同（即二维），最后就是使用特殊的激活函数（文中用的softmax）转换成了0-1的数值，即最后我们需要的的概率。<br>大体结构我手绘了一下…<br><img src="/2019/05/05/看着整整神经网络/3.jpg" alt="示意"></p><p>&emsp;&emsp;可能各位已经看迷糊了，这一统操作我们究竟在干什么？为什么我们这样就可以生成他的概率？<br>&emsp;&emsp;我个人认为这可能就是神经网络最难理解的地方吧。我们在隐藏层内的数据其实并没有任何意义的，我们需要的是根据最后输出的结果对比他的真实情况再去调整W和B的值来让最后的输出的正确类别的概率向1逼近。<br>&emsp;&emsp;也就是说我们没必要去纠结于隐藏层中数据的意义，因为他只是我们得到结果过程中的一个中介，即使它最后的结果荒唐离谱，我们也可以通过后期调参让它逼近正确的数值，这就是一个神经网络学习的过程。<br>&emsp;&emsp;至于W该如何选择它的维度（即选择隐藏层的维度），就需要自己去自己评判了，因为过大和过小都会使模型欠拟合或者过拟合。<br><img src="/2019/05/05/看着整整神经网络/4.png" alt="更换W的参数"><br>至于调参的原理，这就涉及到bp神经网络另一个重要的知识点</p><h1 id="神经网络调参：反向传播"><a href="#神经网络调参：反向传播" class="headerlink" title="神经网络调参：反向传播"></a>神经网络调参：反向传播</h1><p>&emsp;&emsp;前面说到，神经网络的学习过程就在于根据预测结果与正确值的差距进行调参的过程，那么如何进行这个过程呢，这就涉及到bp神经网络的重要内容，反向传播。<br>&emsp;&emsp;何为反向传播，就是我们将我们预测结果与真实值的偏差重新传入神经网络，只不过是从输出层反向传入，再根据得到的结果调节参数。<br>&emsp;&emsp;也就是说我们需要找到一个让参数调整的值，使得这个偏差最小，如何去寻找这个值呢，这里就要使用到梯度下降的原理了<br><img src="/2019/05/05/看着整整神经网络/5.jpg" alt="梯度下降"><br>&emsp;&emsp;很好理解，y轴为偏差，x轴为我们的参数的数值也就是W或B，我们想要让它向最低值逼近，只需要对它求导找到它的梯度（可以理解为就是当前点最大的斜率），然后就让参数以此斜率乘一个数进行加减，至于这个数，称为学习率，如果学习率小了，效率就会降低，太大了有可能超越最低点。<br>&emsp;&emsp;至于这个导数怎么求就涉及到高数的东西了，自己去了解吧，这里直接放原文中的公式吧。</p><script type="math/tex; mode=display">δ_3 = y(真实值)-y</script><script type="math/tex; mode=display">δ_2 = (1-tanh^2z_1)δ_3(W_2)^T</script><script type="math/tex; mode=display">\frac {∂L} {∂W_2} = (a_1)^Tδ_3</script><script type="math/tex; mode=display">\frac {∂L} {∂b_2} = δ_3</script><script type="math/tex; mode=display">\frac {∂L} {∂W_1} = x^Tδ_2</script><script type="math/tex; mode=display">\frac {∂L} {∂b_1} = δ_2</script><p>最后我们计算出的$\frac {∂L} {∂W_2}$，$\frac {∂L} {∂b_2}$，$\frac {∂L} {∂W_1}$，$\frac {∂L} {∂b_1}$再乘学习率就是我们需要参数的改变量了</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;初识神经网络&quot;&gt;&lt;a href=&quot;#初识神经网络&quot; class=&quot;headerlink&quot; title=&quot;初识神经网络&quot;&gt;&lt;/a&gt;初识神经网络&lt;/h1&gt;&lt;p&gt;这篇文章主要对神经网络进行一下简单的概述，毕竟如果把里面的每一个点拉出来都够讲好久&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="简要" scheme="http://yoursite.com/tags/%E7%AE%80%E8%A6%81/"/>
    
  </entry>
  
  <entry>
    <title>openpyxl</title>
    <link href="http://yoursite.com/2019/05/02/openpyxl/"/>
    <id>http://yoursite.com/2019/05/02/openpyxl/</id>
    <published>2019-05-02T14:46:27.000Z</published>
    <updated>2019-05-09T09:30:23.461Z</updated>
    
    <content type="html"><![CDATA[<h1 id="关于Pyhton将数据写入excel表"><a href="#关于Pyhton将数据写入excel表" class="headerlink" title="关于Pyhton将数据写入excel表"></a>关于Pyhton将数据写入excel表</h1><p>今天被强迫性建模，处理数据后又忘记了怎么把数据写入excel表格，现找到了之前的源码复制粘贴，还是在这里记录一下吧以后方便copy。<br><a id="more"></a></p><h2 id="openyxl"><a href="#openyxl" class="headerlink" title="openyxl"></a>openyxl</h2><p>之前找了半天才找到的比较好用的库，先直接上代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def WriteExcel(data, colIndex, sheet):</span><br><span class="line">wb = openpyxl.load_workbook(&quot;路径&quot;)</span><br><span class="line">ws = wb.get_sheet_by_name(sheet)</span><br><span class="line">for i in range(len(data)):</span><br><span class="line">ws.cell(row=i+1, column=colIndex).value = float(data[i])</span><br><span class="line">wb.save(&quot;路径&quot;)</span><br></pre></td></tr></table></figure></p><p>这里写的是把list写入excel的一列，wb.get_sheet_by_name(sheet)中参数是工作簿的名称。<br>cell(row=i+1, column=colIndex).value = float(data[i])中cell函数的参数就是行和列，直接更改他的value属性就可以了。不过要注意的是，row和col最小的取值是1，和平日的大部分索引不同。</p><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>openxyl只能读取xlsx格式的表格<br>错误提示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">InvalidFileException: openpyxl does not support the old .xls file format, please use xlrd to read this file, or convert it to the more recent .xlsx file format.</span><br></pre></td></tr></table></figure></p><h2 id="最后，祈求在我学会神经网络之前不要再让我建模了！！！"><a href="#最后，祈求在我学会神经网络之前不要再让我建模了！！！" class="headerlink" title="最后，祈求在我学会神经网络之前不要再让我建模了！！！"></a>最后，祈求在我学会神经网络之前不要再让我建模了！！！</h2>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;关于Pyhton将数据写入excel表&quot;&gt;&lt;a href=&quot;#关于Pyhton将数据写入excel表&quot; class=&quot;headerlink&quot; title=&quot;关于Pyhton将数据写入excel表&quot;&gt;&lt;/a&gt;关于Pyhton将数据写入excel表&lt;/h1&gt;&lt;p&gt;今天被强迫性建模，处理数据后又忘记了怎么把数据写入excel表格，现找到了之前的源码复制粘贴，还是在这里记录一下吧以后方便copy。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Pyhton" scheme="http://yoursite.com/tags/Pyhton/"/>
    
      <category term="openpyxl" scheme="http://yoursite.com/tags/openpyxl/"/>
    
      <category term="Big-Digital" scheme="http://yoursite.com/tags/Big-Digital/"/>
    
  </entry>
  
  <entry>
    <title>First Blog</title>
    <link href="http://yoursite.com/2019/04/29/First-Blog/"/>
    <id>http://yoursite.com/2019/04/29/First-Blog/</id>
    <published>2019-04-29T12:30:28.000Z</published>
    <updated>2020-08-05T15:28:53.293Z</updated>
    
    <content type="html"><![CDATA[<p>第一篇博客</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;第一篇博客&lt;/p&gt;

      
    
    </summary>
    
    
    
      <category term="First" scheme="http://yoursite.com/tags/First/"/>
    
      <category term="happy" scheme="http://yoursite.com/tags/happy/"/>
    
  </entry>
  
</feed>
